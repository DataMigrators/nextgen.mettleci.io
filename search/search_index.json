{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The latest version can always be found here: https://pages.github.ibm.com/datamigrators/nextgen.mettleci.io/</p> <ul> <li> <p> Learn the basics</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p> </li> <li> <p> Create your first unit tests</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p> </li> <li> <p> Run automated code checks</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p> </li> <li> <p> Automatically build and deploy</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:build","title":"Build","text":"<ul> <li>            Build and deployment pipelines          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:cli","title":"CLI","text":"<ul> <li>            Build and deployment pipelines          </li> <li>            Compliance namespace          </li> <li>            DataStage namespace          </li> <li>            Overview          </li> <li>            The Project Cache Directory          </li> </ul>"},{"location":"tags/#tag:capture","title":"Capture","text":"<ul> <li>            Capturing test data          </li> </ul>"},{"location":"tags/#tag:creating-tests","title":"Creating Tests","text":"<ul> <li>            Capturing test data          </li> <li>            DataStage test specification format          </li> <li>            Getting Started With Testing DataStage Flows          </li> <li>            High Volume DataStage Tests          </li> <li>            Introduction to Data Fabrication          </li> <li>            Recapture a test result baseline          </li> <li>            Test Cases with sparse lookups          </li> <li>            Testing flow with sub-flows          </li> <li>            Testing flows using date/time references          </li> <li>            Testing flows with rejects          </li> <li>            Testing flows with stored procedure stages          </li> <li>            Testing flows with surrogate key generator stages          </li> <li>            Using Test Case Data Fabrication          </li> </ul>"},{"location":"tags/#tag:datastage","title":"DATASTAGE","text":"<ul> <li>            Configuring test data storage          </li> <li>            Creating a DataStage test case          </li> <li>            Editing a DataStage test case          </li> <li>            Excluding columns from tests          </li> <li>            Executing a DataStage test case          </li> <li>            High Volume DataStage Tests          </li> <li>            Migrating test cases from older versions of DataStage          </li> <li>            Row count comparisons          </li> <li>            Selective stubbing          </li> <li>            Troubleshooting          </li> </ul>"},{"location":"tags/#tag:data-fabrication","title":"Data Fabrication","text":"<ul> <li>            Fabrication namespace          </li> <li>            Introduction to Data Fabrication          </li> <li>            Using Test Case Data Fabrication          </li> </ul>"},{"location":"tags/#tag:datastage","title":"DataStage","text":"<ul> <li>            Capturing test data          </li> <li>            DataStage test specification format          </li> <li>            Fabrication namespace          </li> <li>            Getting Started With Testing DataStage Flows          </li> <li>            High Volume DataStage Tests          </li> <li>            Introduction          </li> <li>            Introduction to Data Fabrication          </li> <li>            MettleCI Command Shell          </li> <li>            Recapture a test result baseline          </li> <li>            Sample flow analysis rules          </li> <li>            Test Cases with sparse lookups          </li> <li>            Testing flow with sub-flows          </li> <li>            Testing flows using date/time references          </li> <li>            Testing flows with rejects          </li> <li>            Testing flows with stored procedure stages          </li> <li>            Testing flows with surrogate key generator stages          </li> <li>            UnitTest namespace          </li> <li>            Using Test Case Data Fabrication          </li> <li>            Verifying test results          </li> </ul>"},{"location":"tags/#tag:deploy","title":"Deploy","text":"<ul> <li>            Build and deployment pipelines          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:executing-tests","title":"Executing Tests","text":"<ul> <li>            Getting Started With Testing DataStage Flows          </li> </ul>"},{"location":"tags/#tag:fabrication","title":"Fabrication","text":"<ul> <li>            Fabrication          </li> </ul>"},{"location":"tags/#tag:flow-analysis","title":"Flow Analysis","text":"<ul> <li>            Compliance namespace          </li> <li>            Introduction          </li> <li>            Sample flow analysis rules          </li> </ul>"},{"location":"tags/#tag:flow-analysis","title":"Flow analysis","text":"<ul> <li>            Flow tagging          </li> </ul>"},{"location":"tags/#tag:pipeline","title":"Pipeline","text":"<ul> <li>            Compliance namespace          </li> </ul>"},{"location":"tags/#tag:pipelines","title":"Pipelines","text":"<ul> <li>            Build and deployment pipelines          </li> <li>            DataStage namespace          </li> <li>            Filesystem deployment          </li> <li>            Overview          </li> <li>            Repeatable DataStage Project Deployments          </li> </ul>"},{"location":"tags/#tag:reference","title":"Reference","text":"<ul> <li>            Sample flow analysis rules          </li> </ul>"},{"location":"tags/#tag:running-tests","title":"Running Tests","text":"<ul> <li>            MettleCI Command Shell          </li> <li>            UnitTest namespace          </li> </ul>"},{"location":"tags/#tag:tag1","title":"TAG1","text":"<ul> <li>            Hotfixes          </li> <li>            MettleCI Example Pipeline for DevOps          </li> <li>            Page Template: Update this with your Page Title          </li> </ul>"},{"location":"tags/#tag:tag2","title":"TAG2","text":"<ul> <li>            Hotfixes          </li> <li>            MettleCI Example Pipeline for DevOps          </li> <li>            Page Template: Update this with your Page Title          </li> </ul>"},{"location":"tags/#tag:tag3","title":"TAG3","text":"<ul> <li>            Hotfixes          </li> <li>            MettleCI Example Pipeline for DevOps          </li> <li>            Page Template: Update this with your Page Title          </li> </ul>"},{"location":"tags/#tag:testing","title":"TESTING","text":"<ul> <li>            Configuring test data storage          </li> <li>            Creating a DataStage test case          </li> <li>            Editing a DataStage test case          </li> <li>            Excluding columns from tests          </li> <li>            Executing a DataStage test case          </li> <li>            High Volume DataStage Tests          </li> <li>            Migrating test cases from older versions of DataStage          </li> <li>            Row count comparisons          </li> <li>            Selective stubbing          </li> <li>            Troubleshooting          </li> </ul>"},{"location":"tags/#tag:tagging","title":"Tagging","text":"<ul> <li>            Flow tagging          </li> </ul>"},{"location":"tags/#tag:tags","title":"Tags","text":"<ul> <li>            Compliance namespace          </li> <li>            DataStage namespace          </li> </ul>"},{"location":"tags/#tag:test-results","title":"Test Results","text":"<ul> <li>            Verifying test results          </li> </ul>"},{"location":"tags/#tag:test-cases","title":"Test cases","text":"<ul> <li>            Fabrication          </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"fabrication/fabrication/","title":"Fabrication","text":"","tags":["Fabrication","Test cases"]},{"location":"fabrication/introduction-to-fabrication/","title":"Introduction to Data Fabrication","text":"<p>There are many benefits to using artifically generated data in software unit testing:</p> <ul> <li> <p>It enables the construction of consistent and repeatable tests. Fabricated data is predictable and avoids reliance on external systems like live databases or APIs.  This makes tests deterministic - exhibiting the same behaviour every time they\u2019re run - which is critical for debugging and automation.</p> </li> <li> <p>It can improve test coverage. You can fabricate edge cases, boundary conditions, and unusual scenarios that may not be present in real datasets. This allows you to stress-test logic under various conditions such as empty strings, large inputs, and invalid values.</p> </li> <li> <p>It increases testing speed and efficiency. Tests run faster when they don\u2019t have to query or populate real databases or APIs. The volume of your fabricated data can be optimised to supply no more and no less than is required to demonstrate your code\u2019s correct behaviour, resulting in quicker execution, and consequently faster feedback, when used in CI/CD pipelines.</p> </li> <li> <p>It supports test isolation. Data fabrication ensures that each unit test is self-contained and does not depend on the state of an external system, or the result of other tests. This promotes modular testing and avoids flaky tests caused by the use of shared data.</p> </li> <li> <p>It improves security and privacy. Synthetic data avoids the risks of exposing sensitive or production data in development or test environments.  This is particularly important for DataStage applications which commonly handle PII, health, or financial data.</p> </li> </ul> <p>To deliver these benefits MettleCI provides a comprehensive set of test data fabrication tools which support a wide range of use cases.  You can learn how to use these capabilities here.</p> <p>Where you have additional data fabrication requirements, specific to your industry, organization, or team, MettleCI also enables you to develop your own custom data fabrication capabilities.</p>","tags":["DataStage","Creating Tests","Data Fabrication"]},{"location":"flow-analysis/flow-analysis/","title":"Introduction","text":"<p>Static code analysis is the analysis of a computer program's code without executing that code; In DataStage NextGen this is referred to as Flow Analysis.  This approach contrasts with dynamic program analysis, which is performed on programs during their execution and is achieved in DataStage by creating and running a DataStage Unit Test Case.  </p> <p>DataStage's flow analysis functionality validates one or more DataStage assets against a set of quality criteria defined in Flow Analysis Rules.  MettleCI ships with a set of sample flow analysis rules which provide a basis for customers to develop their own suite of flow analysis rules by either using the shipped rules as supplied, ignoring them, modifying them, or augmenting them with new customer-specific rules as required to meet specific needs.  </p> <p>Flow analysis rules have a variety of applications:</p> <ul> <li>Enforcing coding standards in DataStage flows, improving the consistency, maintainabiity, and security of your solution</li> <li>Identifying external assets, which may require external management</li> <li>Identifying deprecated stages, allowing upgrade effort to be quantified.</li> </ul> <p>Flow analysis rules are grouped into tags</p> <p>Flow anlysis rules are also executable as part  The Sort stage (highlighted) does not comply with an existing Compliance Rule designed to ensure naming standards.  Note that the DataStage custom menu actions shown in this page are installed by following the DataStage Integration Setup instructions.</p>","tags":["DataStage","Flow Analysis"]},{"location":"flow-analysis/flow-rules/","title":"Sample flow analysis rules","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#adjacent-transformers","title":"Adjacent transformers","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary","title":"Summary","text":"<p>Identifies flow designs with adjacent Transformer stages.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description","title":"Description","text":"<p>Although powerful, transformers contain embedded logic that's not immediately obvious when looking at the canvas and reduces a developers ability to immediately understand a flow's logic. Overuse of transformers when more 'verbose' stages could achieve the same outcome can result in greater maintenance overhead.</p> <p>THe presence of Adjacent transformers can be a symptom of a single transformer that has become so complex that a developer has needed to split its logic across two transformers. When this occurs, developers should consider if they could adopt an alternative implementation strategy using multiple standard stages working in conjunction rather than performing all logic within one or more transformer stages.</p> <p>Adjacent transformers can also be a symptom of defect fixes that are clumsily 'tacked on' to an existing job rather than understanding the existing job logic and refactoring accordingly. Making job changes without having to refactor the existing code will degrade job quality and increase maintenance costs over time.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions","title":"Actions","text":"<p>Adjust your flow design so that there are no adjacent transformers. Consider combining adjacent Transformer stages if possible.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#aggregator-stage-not-preceded-by-check-sort-stage","title":"Aggregator stage not preceded by check sort stage","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_1","title":"Summary","text":"<p>The Aggregator stage will only return correct results if the data are presented to the stage is already sorted by the specified aggregation keys.</p> <p>A simple way of verifying this without unneccessarily introducing a maintenance or performance overhead is to include a 'check sort' stage before the aggregator stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_1","title":"Description","text":"<p>This rule perform the following checks:</p> <ol> <li>The Aggregator stage is preceded by a Sort stage</li> <li>The keys of the Aggregator Stage are a subset of those of the Sort Stage</li> <li>The keys of the Sort Stage are set to either <code>(Previously Sorted)</code> or <code>(Previously Grouped)</code></li> </ol>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_1","title":"Actions","text":"<p>Add a Check Sort stage to before Aggregator stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#audit-annotation","title":"Audit annotation","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_2","title":"Summary","text":"<p>Identifies where sensitive information may be present in DataStage flow and pipeline annotations.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_2","title":"Description","text":"<p>This rule scans all annotations for instances of the words <code>hostname</code>, <code>password</code>, <code>organisation</code> (<code>en-UK</code> spelling) or <code>organization</code> (<code>en-US</code> spelling).</p> <p>Note that it's impossible to use regular expressions to detect a password, host name, or organisation, but assuming one of these words is found the likelihood of the actual sensitive information being present is high.</p> <p>Like all MettleCI Compliance Rules, this rule is provided as an example intended for you to adapt to suit your needs.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_2","title":"Actions","text":"<p>Remove the sensitive information from your flow or pipeline annotation.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#avi-stage-not-preceded-by-sort-stage","title":"AVI Stage Not Preceded by Sort Stage","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_3","title":"Summary","text":"<p>Identifies where an AVI stage is not preceded by a Sort operator,</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_3","title":"Description","text":"<p>This rule validates that the flow matches the following criteria: 1. All AVI Stages are preceded by a Sort Stage (the sort mode is irrelevant), and 1. The columns specified in the rule definition are the first sort keys defined in the preceding Sort Stage.</p> <p>When customising this rule you should use the <code>addressSortProperty</code> variable to specify which address property/properties you want to ensure your data is sorted by.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_3","title":"Actions","text":"<p>Ensure the <code>addressSortProperty</code> variable in the rule defines the address property/properties you want your data to be sorted by.</p> <p>Adjust your job design so that your AVI stage is immediately preceded by a Sort stage. The sort mode is irrelevant.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#avi-stage-requires-country-code","title":"AVI Stage Requires Country Code","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_4","title":"Summary","text":"<p>Always provide a column mapped to the country code when using the AVI stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_4","title":"Description","text":"<p>Providing the country code helps keep address search results country-specific, and optimises the performance of the algorithm.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_4","title":"Actions","text":"<p>Ensure you map a column to the country code in your AVI stage configuration.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#connection-contains-parameter-references","title":"Connection contains parameter references","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_5","title":"Summary","text":"<p>Identifies Connections created by the migration from legacy DataStage to NextGen that contain references to flow parameters.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_5","title":"Description","text":"<p>https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=datastage-migrating#reference_fbm_gww_yqb__post-mig__title__1</p> <p>During the process of migrating legacy DataStage projects to NextGen, connection details and credentials are moved from the flow into a separate Connection, which may be also used outside of a DataStage context. When the connection details are parameterised those parameter references are copied into the Connection object. These must then be replaced with real credentials so it can be tested and saved.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_5","title":"Actions","text":"<p>Replace the parameter references with actual connection details.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#connection-not-using-secrets-vault","title":"Connection Not Using Secrets Vault","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_6","title":"Summary","text":"<p>Identifies Connections that don't store their credentials to use secrets in a vault.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_6","title":"Description","text":"<p>Connections have the option of retrieving their credentials from a secret in a vault. If this option isn't taken the credential values are stored in plain text when the connection object is exported from the project, despite being displayed in the user interface as encrypted.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_6","title":"Actions","text":"<p>Update the connection, setting the Input Method as 'Use secrets from a vault' .</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#dataset-not-using-same-partition","title":"DataSet Not Using Same Partition","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_7","title":"Summary","text":"<p>Identifies Data Sets not using the <code>Same</code> partitioning method</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_7","title":"Description","text":"<p>This rule identifies flow data sets which don't use the <code>Same</code> partitioning method.</p> <p>This is an IBM development tip documented here: Sorting data - IBM Documentation </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_7","title":"Actions","text":"<p>When reading from Data Sets maintain your sort order by using <code>Same</code> partitioning method.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#database-connector-not-using-auto-generate-sql","title":"Database Connector Not Using Auto Generate SQL","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_8","title":"Summary","text":"<p>Identifies database connections that don't use the <code>Auto Generate SQL</code> option.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_8","title":"Description","text":"<p>In InfoSphere Information Governance Catalog, the schema and database table name of the imported database must be the same as the schema and database table name in the stage.</p> <p>You can generate default SQL statements to read from and write to data sources. Alternatively, you can specify SQL statements manually that read from and write to data sources. SQL that you specify might contain certain DBMS vendor-specific constructs that can't be parsed correctly in lineage analysis.</p> <p>As a result, the relationships between stages and data sources and between stages and other stages might be incorrect.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_8","title":"Actions","text":"<p>Update the connector to use Auto Generate SQL.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#database-query-from-file","title":"Database query from file","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_9","title":"Summary","text":"<p>Ensures if Database Connectors and Stages reads SQL statement from file.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_9","title":"Description","text":"<p>Developers occasionally wants to keep the SQL statements flexible, and use external file for SQL statements. However, SQL statements from external files don't go to the metadata repository where they can be used for lineage analysis. These SQL statements are also not included in operational metadata XML files.</p> <p>This can make the flow harder to maintain as it contains external objects, and failure when deploying to a different project of server.</p> <p>It's recommended for developers to use parameters instead of an external file if they wish to benefit from the flexibility of using SQL statements.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_9","title":"Actions","text":"<p>Update the Connector to use the <code>Read select statement from file</code> property.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#database-row-limit","title":"Database row limit","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_10","title":"Summary","text":"<p>Identifies Connectors Stages with a configured Database Row Limit.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_10","title":"Description","text":"<p>Row limits set on the Database stages are options developers might use during development and unit testing to allow faster development. Leaving these options set may have unintended consequences should the flow be deployed into test and production environments.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_10","title":"Actions","text":"<p>Remove the configured row limit.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#database-table-references-are-fully-qualified","title":"Database table references are fully qualified","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_11","title":"Summary","text":"<p>Identifies where database table references aren't fully qualified.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_11","title":"Description","text":"<p>Database table references can sometimes cause problems when migrating from Development to downstream QA and Production environments as they will likely be communicating with different databases to those used in Development. Those databases may well have different configurations to those that were iused to develop the the flow and there may arise some confusion about which schemas or other database objects are being referenced by the flow.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_11","title":"Actions","text":"<p>Ensure all database object references used by your flow are fully qualified. For example, <code>{schema}.{tablename}</code></p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#data-format-in-annotation","title":"Data format in annotation","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_12","title":"Summary","text":"<p>Identify whether the a flow annotation contains instances of particular arbitrary text.  This example rule looks for dates.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_12","title":"Description","text":"<p>Identify whether the a flow annotation contains instances of particular arbitrary text. You can modify this rule to get it to seek out any text your team wants to prevent passing compliance.  This example rule looks for text matching the following date formats:</p> Format Description <code>YYYY-MM-DD</code>, <code>YYYY/MM/DD</code>, <code>YYYY MM DD</code> YYYY-MM-DD format with <code>/</code> or <code>-</code> as a delimiter <code>YYYY-MMM-DD</code>, <code>YYYY/MMM/DD</code>, <code>YYYY MMM DD</code> Month format can be MM, MMM, MMMM, or MON <code>YYYY-MMMM-DD</code>, <code>YYYY/MMMM/DD</code>, <code>YYYY MMMM DD</code> MMM and MMMM are Excel Syntax <code>DD-MM-YYYY</code> Similar to YYYY-MM-DD <code>MM-DD-YYYY</code> Similar to YYYY-MM-DD (common US date format)","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_12","title":"Actions","text":"<p>Remove or alter the text that references a date.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#db2-with-no-non-recovery-load","title":"Db2 with no non-recovery load","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_13","title":"Summary","text":"<p>Identify Db2 Stages using bulk load with Non Recovery Load set to 'No'.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_13","title":"Description","text":"<p>This rule warns developers using Db2 bulk load with Non Recovery Load set to 'No' which could require the restoration of the Db2 database if it crashes during the execution.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_13","title":"Actions","text":"<p>User dependent.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#debug-row-limit","title":"Debug row limit","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_14","title":"Summary","text":"<p>Identifies row limits in debug stages (Peek, Sample, Tail)</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_14","title":"Description","text":"<p>Debug stages are options developers might use during development to enable debugging and testing. Leaving these options set may have unintended consequences should the flow be deployed into test and production environments.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_14","title":"Actions","text":"<p>Remove either the Debug Row Limit setting, or remove the Stage within which it is set.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#default-stage-naming","title":"Default stage naming","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_15","title":"Summary","text":"<p>Default stage names for flows and automation pipelines.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_15","title":"Description","text":"<p>Stages should be given meaningful names, to improve readability and reduce bug fixing effort. The use of default (DataStage canvas-generated) names (e.g. <code>Link123</code>) suggests that flows might still be ready for promotion to downstream testing and production environments.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_15","title":"Actions","text":"<p>Ensure that all flows, Sequences, Stages, and Links are given user-chosen names (not just defaults) which align with your naming standards. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#duplicate-file-references","title":"Duplicate file references","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_16","title":"Summary","text":"<p>Verifies that a sequential file is\u2026  - only referenced once in a DataStage flow's sequential file, complex flat file, or data set stages, and  - not simlutaneously being read and written in the same DataStage flow</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_16","title":"Description","text":"<p>Sometimes developers will copy and paste stages rather than adding a new stage from the palette. In doing so there is a risk that the developer may not update the reference to the filename property which may result in incorrect processing being performed by the DataStage flow.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_16","title":"Actions","text":"<p>Ensure that a file is only referenced once in a DataStage flow.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#duplicate-stage-names","title":"Duplicate stage names","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_17","title":"Summary","text":"<p>Detect duplicate Stage names in a flow.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_17","title":"Description","text":"<p>This can create issues with Unit Testing and potentially other Information Server tools. The chances of occurrence is rare, but not impossible, especially with assets imported from legacy DataStage instances.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_17","title":"Actions","text":"<p>Adjust your flow's stage names to remove duplicates.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#file-reference-missing-required-parameter","title":"File reference missing required parameter","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_18","title":"Summary","text":"<p>Ensures that all file stages must use variables for determining paths.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_18","title":"Description","text":"<p>Developers occasionally hard code file paths while debugging or developing. </p> <p>These will cause job failures when deploying to a different project or server. </p> <p>This problem is usually solved by using a project-specific variable to define the base path for file based stages such as datasets. This compliance rule will identify hard coded file paths by checking that the pathname includes at least one predefined path parameter.  You can customise the predefined path parameters (and the stages for which they can be used) by modifying the <code>pathParameters</code> variable.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_18","title":"Actions","text":"<p>Use a project-specific variable to define the base path for file based stages.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#file-row-limit","title":"File row limit","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_19","title":"Summary","text":"<p>Identifies row limits in file-based stages (sequential file, complex flat file).</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_19","title":"Description","text":"<p>Row limits set on the file stages are options developers might use during development and unit testing to allow faster development. Leaving these options set may have unintended consequences should the flow be deployed into test and production environments.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_19","title":"Actions","text":"<p>Remove the row limit on identified Stages.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#flow-naming","title":"Flow naming","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_20","title":"Summary","text":"<p>Checks whether the flow name matches one of a list of prohibited name patterns.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_20","title":"Description","text":"<p>Developers will occasionally work on a copy or Work-In-Progress version of a flow but forget to rename it before pushing it to Git. This simple compliance test makes these mistakes visible at check-in time by determining whether it matches a list of known prohibited name patterns (such as <code>CopyOf*</code>, for example.)</p> <p>See the documentation on asset naming standards for the default list of naming signatures on the prohibited list.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_20","title":"Actions","text":"<p>Rename the flow to align with your coding standards and remove ambiguity. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#flow-parameter-missing-default-value","title":"Flow parameter missing default value","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_21","title":"Summary","text":"<p>Identify flow parameters within a default value.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_21","title":"Description","text":"<p>It is good practice to consider the use of default parameter values to protect flows from unintended behaviour when invoked without providing all required parameter values.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_21","title":"Actions","text":"<p>Ensure all your flow parameters have a default value.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#flow-parameter-naming","title":"Flow parameter naming","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_22","title":"Summary","text":"<p>Identifies where naming standards for flow parameters and parameter sets are breached.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_22","title":"Description","text":"<p>Identifies where naming standards for flow parameters Parameter sets are breached </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_22","title":"Actions","text":"<p>Ensure your flow parameters and parameter sets are named according to your standards. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#flow-parameter-missing-default-value_1","title":"Flow parameter missing default value","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_23","title":"Summary","text":"<p>Identify flow parameters that are not in a parameter set.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_23","title":"Description","text":"<p>It is good practice to consider the use of Parameter Sets to store flow Parameters.  Parameter Sets help to make your flows more flexible and reusable across environments.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_23","title":"Actions","text":"<p>Ensure all your flow parameters are included in a parameter set.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#flow-using-random-function","title":"Flow using random function","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_24","title":"Summary","text":"<p>Identifies Transformers (or other stages) using DataStage functions which produce non-deterministic output.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_24","title":"Description","text":"<p>DataStage functions <code>Rand()</code>, <code>Random()</code> and <code>Srandom()</code> are examples of functions which produce non-deterministic output. Columns whose values are dependent upon these functions will produce unpredictable results which can't be used in DataStage test cases.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_24","title":"Actions","text":"<p>Parameterise your flow designs by specifying the required non-deterministic value as a flow parameter. This value will then be supplied with a fixed value at invocation time to guarantee a predictable unit test result.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#flow-using-transformer-surrogate-key","title":"Flow using transformer surrogate key","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_25","title":"Summary","text":"<p>Identifies flow designs that use the <code>NextSurrogateKey()</code> call in a Transformer stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_25","title":"Description","text":"<p>Although powerful, transformers contain embedded logic that's not immediately obvious when looking at the canvas and reduces a developers ability to immediately understand a flow's logic. Overuse of transformers when more 'verbose' stages could achieve the same outcome can result in greater maintenance overhead.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_25","title":"Actions","text":"<p>Adjust your flow design to use a Surrogate Key Generator stage instead.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#hard-coded-db-credentials","title":"Hard-coded DB credentials","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_26","title":"Summary","text":"<p>Ensures that all Database Connectors must use variables for location and credentials</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_26","title":"Description","text":"<p>Developers occasionally hard code database credentials into their flows. These will cause job failures when deploying to a different project or server.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_26","title":"Actions","text":"<p>Parameterise the identified hard-coded values.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#hard-coded-file-paths","title":"Hard-coded file paths","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_27","title":"Summary","text":"<p>Ensures that all File Stages must use variables for determining paths</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_27","title":"Description","text":"<p>Developers occasionally hard-code file paths while debugging or developing. These will cause job failures when deploying to a different project or server. This problem is usually solved by using a project specific variable to define the base path for file based stages such as datasets. This compliance rule will identify hard-coded file paths by checking that the pathname includes at least one predefined path parameter.</p> <p>The predefined path parameters and the stages for which they can be used with can be customised by modifying the <code>pathParameters</code> variable.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_27","title":"Actions","text":"<p>Parameterise the identified hard-coded file paths.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#join-partition-mismatch-with-join-key","title":"Join partition mismatch with join key","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_28","title":"Summary","text":"<p>Reports Join stages where the join key does not match the partitioning of the input links.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_28","title":"Description","text":"<p>In the parallel DataStage engine, data is internal split into separate partitions in order to run a number of smaller operations concurrently. This results in faster and more efficient processing, especially when sorting and joining data.</p> <p>The Hash, Range and Modulus partitioning methods use column values to determine the partition within which each record is placed. For the Same method the actual partitioning method is propagated from the upstream selection, and so we need to traverse up to the preceding stages until a specific method is selected (or until we reach the data source and can go no further).</p> <p>For a pair of records in the left and right links to join, they must both be placed in the same partition.</p> <p>If the columns used to determine the partition allocation are not in alignment with the keys used to join the two sources of data, records that are supposed to join may be in different partitions, and therefore not join as expected.</p> Join Key Join Partition Result key1 key2 fail key1, key2 key2 fail key1, key2 key1, key2 pass key1, key2 key1 pass key1 key1 pass","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_28","title":"Actions","text":"<p>Ensure partitioning methods and columns are in alignment with selected join keys.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#link-sort","title":"Link sort","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_29","title":"Summary","text":"<p>Identifies link sorts.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_29","title":"Description","text":"<p>Developers where possible should utilise the sort stage rather than utilising the link sort facility. The sort stage allows more options for controlling the behaviour of the sort operation, appears as a distinct component in monitoring tools, and better communicates a flow's design intentions to other DataStage developers.</p> <p>This rule identifies flows that utilise implicit sorts on a link which may prevent developers from taking advantage of some opportunities for performance optimisation, and may reduce the easy maintainability of your Flows. Both link Sorts and explicit Sorts generate the same underlying orchestrate operators, however the explicit Sort stage offers the following options which are ot available on a link Sort:</p> <ul> <li>Sort Key Mode</li> <li>Create Cluster Key Change Column</li> <li>Create Key Change Column</li> <li>Output Statistics</li> <li>Sort Utility</li> <li>Restrict Memory Usage</li> </ul>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_29","title":"Actions","text":"<p>Replace link sorts with an explicit Sort stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#log-column-values","title":"Log column values","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_30","title":"Summary","text":"<p>Identify Stages configured to Log column values on first row error</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_30","title":"Description","text":"<p>The Log column values on first row error property is a feature used for debugging purpose. It outputs error rows in plain text to your DataStage job log.</p> <p>This creates the potential for error rows containing sensitive information to be divulged to unauthorised parties.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_30","title":"Actions","text":"<p>Disable the Log column values on first row error property.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#lookup-failure","title":"Lookup failure","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_31","title":"Summary","text":"<p>Identifies Lookup Stages with Lookup set to Fail.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_31","title":"Description","text":"<p>Lookup stages have a \u2018Condition Not Met\u2019 setting which describes what happens when a key lookup fails.  Options are:</p> <ul> <li>Continue - The fields from that link are set to NULL if the field is nullable, or to a default value if not, and processing continues</li> <li>Drop - Drops the row and continues with the next lookup</li> <li>Fail - Causes the job to issue a fatal error and stop</li> <li>Reject - Sends the row to the reject link</li> </ul> <p>By default Lookup stages are configured to Fail should a key lookup fail. This unexpected job abort is, in most cases, not the behaviour the developer intended. These lookup failures should be handled gracefully in the DataStage job.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_31","title":"Actions","text":"<p>Reconfigure your lookup stage so that reference links are not set to abort the job should a key lookup fail.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#one-data-flow","title":"One data flow","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_32","title":"Summary","text":"<p>Verifies that there is only one distinct data flow graph defined in a DataStage flow asset.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_32","title":"Description","text":"<p>A DataStage flow has no restrictions on the number of independent data flows that can be created on its canvas. Ideally a DataStage flow asset should be created for each distinct data flow graph to separate concerns and allow the calling application to manage the execution order most efficiently.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_32","title":"Actions","text":"<p>Split your flow so that each separate data flow graph is defined within its own dedicated flow asset. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#oracle-connector-not-using-partition-read","title":"Oracle connector not using partition read","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_33","title":"Summary","text":"<p>Identify Oracle Stages not configured to use partitioned reads.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_33","title":"Description","text":"<p>When an Oracle Connector Stage running in a Parallel Flow is configured to read data in parallel mode each node of the connector operator reads a distinct subset of data from the source table.  It achieves this by running a subtly modified <code>SELECT</code> statement on each node. The combined set of rows from all of the queries is the same set of rows that would be returned if the unmodified user-defined <code>SELECT</code> statement were run once on one node.  This approach requires that you configure your Oracle Connector Stage appropriately, and can deliver significant performance improvements over using a single-node read operation. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_33","title":"Actions","text":"<p>Ensure that you configure your Oracle Connector Stage appropriately.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#password-parameter-type-not-encrypted","title":"Password parameter type not encrypted","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_34","title":"Summary","text":"<p>Flow parameters with names that suggest they will be used for supplying passwords should be set to use the type \"Encrypted\".</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_34","title":"Description","text":"<p>Job parameters with names that suggest they will be used for supplying passwords should be set to use the type Encrypted.  If they are not encrypted then their plain text contents could present a security risk.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_34","title":"Actions","text":"<p>Set flow parameters used for supplying passwords to use the type Encrypted.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#orchestration-pipeline-uses-hard-coded-parameter-value","title":"Orchestration pipeline uses hard-coded parameter value","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#rationale","title":"Rationale:","text":"<p>In orchestration pipelines, a Run DataStage Job node with parameters should have those parameters  supplied from the pipeline definition itself, to ensure the pipelines and individual jobs operate conistently.</p> <p>In orchestration pipelines, parameters may come from 1 of 5 sources, as displayed in the configuration window:</p> <ul> <li>Default job parameter configuration - the default source. This parameter is known as being 'linked' to the child job<ul> <li>Note: As a key point of difference between Orchestration pipelines and Sequence Jobs in DataStage Classic, these parameter references  are not stored in the orchestration pipeline asset, and are therefore invisible to this model</li> </ul> </li> <li>Enter Value - a hard-coded value passed in from the orchestration pipeline to the child DataStage job (known internally as 'SELECT_RESOURCE')<ul> <li>Select From Another Node - a property from another orchestration pipeline node, e.g.: return status, stand output, etc.  ('SELECT_FROM_NODE')</li> </ul> </li> <li>Assign Pipeline Parameter - allows the Pipeline to control the input of the childe DataStage job at runtime ('FLOW_PARAM')</li> <li>Enter Expression - Create a more complex input value, using a builder that offers a number of string, date/time and conversion functions ('EXPRESSION')</li> </ul>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#orchestration-pipeline-contains-unmigrated-job-routine","title":"Orchestration pipeline contains unmigrated job routine","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_35","title":"Summary","text":"<p>Identifies stages in a migrated orchestration pipeline that unresolved placeholders for Server Routine conversion</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_35","title":"Description","text":"<p>Server Routines in DataStage Classic sequence jobs are not supported in Cloud Pak Data. The migration process converts the Job Routine Activity stage into what looks like a Run Bash Script stage, but is actually just a placeholder.</p> <p>Once a bash script has been entered into the UI, saving the job converts this stage into a normal CPD Run Bash Script stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#orchestration-pipeline-not-restartable","title":"Orchestration pipeline not restartable","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_36","title":"Summary","text":"<p>Identifies orchestration pipeline jobs that are not \"restartable\", ie: if a job run partially succeeds before aboorting, re-running the job will skip the initial successful stages and re-commence from the previous point of failure.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#rationale_1","title":"Rationale","text":"<p>The concept of restartability in orchestration pipelines is a little more complex than it was in DataStage Classic.</p> <p>First, a job has to create a data cache, and then it has to decide to use the previously created cache. The options and their associate behaviour is documented here.</p> <p>This rule identifies the following scenarios:</p> <ul> <li>If the job default Cache Mode is set to <code>Never</code> this tells CPD to ignore previously created caches,      which forces it to run again from the beginning.</li> <li>If every stage that has the option to create a cache either has it turned off, or has its Creation/Copy      Mode set to Overwrite, there are no checkpoints to use in the case of failure,     which again will force a full run from the start of the Pipeline</li> </ul>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#prohibited-stages","title":"Prohibited stages","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_37","title":"Summary","text":"<p>Verifies that a job does not contain one or more prohibited stages.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_36","title":"Description","text":"<p>Some projects may wish to restrict certain stages from production ready jobs. For example, peek stages are usually included by developers as a debugging aid and therefore should be removed after initial development.</p> <p>The list of prohibited stages is easily configurable, and is currently defined as:</p> <pre><code>// File\n   'PxzOSFile',                 // zOSFile\n// Packs\n   'EssbaseConnectorPX',        // Essbase Connector\n   'PS_HRYPX',                  // Hierarchy for PeopleSoft Enterprise\n   'PeoplesoftOnePX',           // JD Edwards EnterpriseOne\n   'ORAAppsPX',                 // Oracle Applications Direct Access\n   'orahrchy_PX',               // Oracle Applications Hierarchy\n   'SALESFORCEJCConnectorPX',   // Salesforce\n   'Siebel_BCPX',               // Siebel BC Pack\n   'Siebel_DAPX',               // Siebel DA Pack\n   'Siebel_EIMPX',              // Siebel EIM Pack\n// Processing\n   'PxSVTransformer'            // BASIC Transformer\n</code></pre>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_35","title":"Actions","text":"<ul> <li>Redesign your job to avoid the use of prohibited stages, or</li> <li>Update the list of prohibited stages to permit stages you wish to permit.</li> </ul>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#range-lookup-incorrectly-configured","title":"Range lookup incorrectly configured","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_38","title":"Summary","text":"<p>Checks that range lookups are correctly configured.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_37","title":"Description","text":"<p>When setting up a range lookup, DataStage will build the lookup table based on the primary key fields but not the range fields. If the range option is enabled on the reference link, this will result in duplicates being ignored and the range lookup won't work as expected.</p> <p>This rule will check that range lookups have been configured to allow duplicates on the reference link and prevent unexpected results due to an incorrectly configured stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_36","title":"Actions","text":"<p>Ensure range lookups are configured to allow duplicates on the reference link.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#redundant-sort","title":"Redundant sort","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_39","title":"Summary","text":"<p>Identifies unnecessary sort opertions within Job designs.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_38","title":"Description","text":"<p>Sorting record data is the most expensive operation that can be performed by DataStage's Parallel Engine. While most experienced developers will arrange job logic to minimise the number of required sorts, there are valid patterns that use adjacent sort stages in conjunction with key change and cluster key flags.</p> <p>This rule will identify adjacent sort stages which do not create key change, cluster key or have keys flagged as Don't Sort. Where adjacent sort stages are discovered, one will either be redundant and should be removed or a developer has introduced a logic error by not including a key change, cluster key or Don't sort flag.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_37","title":"Actions","text":"<p>Remove redundant sorts.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#sql-in-db-connectors","title":"SQL in DB connectors","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_40","title":"Summary","text":"<p>Identifies DB Connectors which use an SQL to filter source data</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_39","title":"Description","text":"<p>Both legacy Enterprise Database Stages and Connector Stages provide a number of mechanisms for specifying the means of specifying the data they should retrieve:</p> <p>| Query mode | Description | Behaviour |  |\u2014\u2014------\u2014\u2014\u2014-|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014|  | User Query    | A custom query specified in the Stage properties                  | Rule fails |  | SQL Builder   | Construct a custom SQL statement with a user interface            | Rule fails |  | SQL File      | A file containing an SQL query is loaded and executed at runtime. | Rule fails |  | Table Query   | Select the name of the table to use from a drop-down list         | Rule fails if SELECT or FILTER clauses specified  |  | Generated     | Underlying SQL is generated by the Stage at runtime               | Rule fails if WHERE or OTHER clauses specified    |</p> <p>This rule checks for the use of hard-coded SQL in properties of Database Enterprise Stages and Connector Stages. Development teams may choose to avoid using hard-coded SQL to filter data and instead opt to use downstream stages to process database output.</p> <p>This rule only permits generated SQL where there are no other clauses specified in connector with that option. Any use-defined SQL, generated SQL with additional clauses or query file inputs will fail. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_38","title":"Actions","text":"<p>Configure your Enterprise Database Stages and Connector Stages to use an approach which does not necessitate the use of hard-coded SQL.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#schema-files-are-referenced","title":"Schema files are referenced","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_41","title":"Summary","text":"<p>Detect use of Schema Files.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_40","title":"Description","text":"<p>In the Upgrade scenario, Schema Files are file system assets that need to be managed and migrated separately.  This rule identifies references to schema files which you will need to manage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_39","title":"Actions","text":"<p>Ensure your schema files are governed in the same manner as your other DataStage assets. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#select-in-custom-sql","title":"\"Select *\" in custom SQL","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_42","title":"Summary","text":"<p>Identifies where \"select *\" syntax has been used in custom SQL code in database Connectors.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_41","title":"Description","text":"<p>Using Select * in custom SQL code may be indicative of \u2026</p> <ul> <li>SQL code that has been directly lifted from a SQL editor and inserted in to DataStage without further analysis, or</li> <li>Code that ignores future changes to the source and possibly produces misleading errors in the DataStage logs</li> </ul>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_40","title":"Actions","text":"<p>Modify your Connector Stages' SELECT statements to use explicit column names. </p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#sequential-file-read-using-same-partitioning","title":"Sequential file read using same partitioning","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_43","title":"Summary","text":"<p>Avoid reading from Sequential Files using the 'Same' partitioning method.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_42","title":"Description","text":"<p>Avoid reading from sequential files using the 'Same' partitioning method.  Unless you have specified more than one source file this will result in the entire file being read into a single partition, making the entire downstream flow run sequentially unless you explicitly re-partition.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_41","title":"Actions","text":"<p>Configure your Sequential File Stage to use a partitioning method other than 'Same'.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#sequential-file-with-reject-mode-not-set-to-fail","title":"Sequential file with reject mode not set to fail","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_44","title":"Summary","text":"<p>Identifies Sequential Files with a Reject Mode not set to fail</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_43","title":"Description","text":"<p>Some scenarios may require you to identify where an input Sequential File contains errors and cease processing.  In this case you would set your Sequential File\u2019s Reject Mode to fail.  This rule assist this process by identifying when the Reject Mode is set to Continue or Save.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_42","title":"Actions","text":"<p>Set your Sequential File\u2019s Reject Mode appropriate to your needs.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#sort-after-join-stage","title":"Sort after join stage","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_45","title":"Summary","text":"<p>Identifies potentially redundant sorting (a sort stage or link sort) situated immediately after a Join stage</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_44","title":"Description","text":"<p>Sorting is resource intensive, and when used excessively can impact performance.  This rule identifies Jobs that sort the output of a Join stage, which is a potentially redundant operation.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_43","title":"Actions","text":"<p>Review the Job design and remove the Sort if redundant.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#stage-naming","title":"Stage naming","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_46","title":"Summary","text":"<p>Stage naming standards for DataStage flows and orchestration pipelines.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_45","title":"Description","text":"<p>Verifies that a Flow's stage names match the specified naming standards. All naming standards are based on example asset naming standards and are expected to be customised to customer requirements.</p> <p>The supplied naming standards are designed to reduce the amount of effort required for developers to understand what a Flow is doing without needing to inspect each stage on the Flow canvas and to quickly identify Flow stages from text based messages such as logged errors and compliance rule failures.</p> <p>In most cases, this is achieved by having a prefix that reflects the stage type. However, for stages such as joins and funnels, the stage names provide additional information which cannot be determined without inspecting the stage in question (ie. inner/left/right join).</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_44","title":"Actions","text":"<p>Ensure your flow stages or orchestration pipeline DataStage components are named according to your naming standards.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#system-time-dependency","title":"System time dependency","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_47","title":"Summary","text":"<p>Identifies the use of system time functions in the job</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_46","title":"Description","text":"<p>Flows with date logic that reads from the system time make it difficult to perform repeatable/automated testing. Instead of using system date functions, it is recommended that the current date be passed in as a Flow parameter or derived from one of the <code>DSJobStart*</code> macros. During unit testing, the Flow parameter or <code>DSJobStart*</code> macro can be set to a fixed value. The actual job results can then be compared with the expected results without needing to account for differences due to system time.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_45","title":"Actions","text":"<p>Replace the reference to the system time or date with a Flow Parameter of the appropriate type.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#too-many-stages","title":"Too many stages","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_48","title":"Summary","text":"<p>Identify whether a flow has too many stages.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_47","title":"Description","text":"<p>A flow design that contains too many stages becomes hard to maintain.  Not only is it difficult to follow the logic but even physically moving around it and finding specific logic is hard.</p> <p>This rule detects when a Flow or local Sub-flow exceeds a globally configurable stage limit.  The default 'stageLimit' parameter is set to 25 but can be easily adjusted as required.  Stages contained in shared Subflows are excluded from the stage count.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_46","title":"Actions","text":"<p>Split your flow into multiple flows, or remove unnecessary Stages.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#transformer-uses-abort-after-rows","title":"Transformer uses abort after rows","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_49","title":"Summary","text":"<p>Detect an 'Abort after rows' greater than 0 setting in a Parallel Transformer.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_48","title":"Description","text":"<p>This rule detects Parallel Transformer with an 'Abort after rows' setting greater than 0. This could be considered bad practice in some organisations as rows unexpectedly appearing on a given link could be more gracefully handled by introducing an error-handling process which does not cause a job to abort.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_47","title":"Actions","text":"<p>Set the 'Abort after rows' value to 0 and provide downstream logic to report and handle the erroneous rows.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#transformer-with-unreferenced-stage-variable","title":"Transformer with unreferenced stage variable","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_50","title":"Summary","text":"<p>Identifies Transformer Stage with an unreferenced Stage Variable (including Loop Variables).</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_49","title":"Description","text":"<p>A Transformer can be resource intensive, and having an unreferenced (i.e. unused) Stage/Loop variable\u2026</p> <ul> <li>can be an unnecessary waste of resources,</li> <li>can cause confusion to flow maintainers, and</li> <li>may present a security vulnerability.</li> </ul> <p>This rule identifies forgotten/unreferenced Stage/Loop variables.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_48","title":"Actions","text":"<p>Remove the unreferenced Stage/Loop variable.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#unencrypted-database-passwords","title":"Unencrypted database passwords","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_51","title":"Summary","text":"<p>Identifies connector stages which do not use encrypted passwords</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_50","title":"Description","text":"<p>Unencrypted passwords are a security risk and should be avoided. </p> <p>In DataStage all hard-coded password values are encrypted, and references to parameters are in plain text. This rule, therefore, simply needs to exclude:</p> <ul> <li>encrypted values, i.e.: <code>{dsnextenc}</code></li> <li>a parameter that is set as encrypted</li> <li>a reference to a parameter set, as the flow configuration does not store the details of parameters inside parameter sets</li> </ul>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#unique-sort","title":"Unique sort","text":"","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#summary_52","title":"Summary","text":"<p>Identifies unique sorts.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#description_51","title":"Description","text":"<p>Unique sorts are not visually represented on the DataStage canvas. Ideally the developer should use the Remove Duplicates stage so that this can be visually communicated with other developers. This rule will identify sort stages which have the <code>Allow Duplicates</code> property set to false.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-rules/#actions_49","title":"Actions","text":"<p>Replace unique sorts with a Remove Duplicates stage.</p>","tags":["DataStage","Flow Analysis","Reference"]},{"location":"flow-analysis/flow-tagging/","title":"Flow tagging","text":"","tags":["Flow analysis","Tagging"]},{"location":"mettleci-cli/command-shell/","title":"MettleCI Command Shell","text":"<p>The DataStage NextGen MettleCI Command Line Interface (referred to as MCIX) is available from either your Windows command line or Unix shell.  It provides one way of accessing MettleCI's build and deployment functions, and supports two different modes of operation: console or command line.  MettleCI commands accept various parameters which can optionally be sourced from a command file.</p> Note <p>Note that the <code>mcix</code> command is the CPD-compatible equivalent to the Classic MettleCI Command Line Interface for DataStage v11.x which is invoked with <code>mettleci</code> (UNIX) or <code>mettleci.cmd</code> (Windows).  The <code>mettleci</code> command is not available in DataStage NextGen, and so the <code>mcix</code> command should be used instead.</p>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/command-shell/#console-mode","title":"Console Mode","text":"<p>To enter console mode start the MettleCI Command Line Interface by entering <code>mcix</code> (UNIX) or <code>mcix.cmd</code> (Windows). </p> <pre><code>C:\\&gt; mcix.cmd\nMettleCI Command Line (build 1234)\n(C) 2018-2025 Data Migrators Pty Ltd\nEnter [namespace] [command] [options]\nor 'help' for more information, 'exit' or 'quit' to leave.\nmcix&gt;\n</code></pre> <p>In console mode MettleCI prints a command prompt and waits for a command. Each command is processed without exiting MettleCI. You may need to provide authentication options for a commands which invoke functionality in third party systems.  You can enter help to get assistance, or exit the console mode by entering exit, or quit at the prompt.</p>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/command-shell/#command-mode","title":"Command mode","text":"<p>In command mode you can enter commands one at a time at your operating system's command line. Start each command (omitting the quotes) with <code>mcix</code> (unix) or <code>mcix.cmd</code> (Windows) followed by a namespace and command, then the parameters.</p> <p></p> <p>Some of the available commands (listed below) use IBM DataStage client components, and so are platform specific.  For example, job compilation requires access to Windows-only components, and so will not be supported on Unix environments.  The MettleCI command is followed by a namespace, which groups a collection of build and deployment operations.  Each of these commands accepts a number of mandatory and/or optional parameters. </p> <pre><code>mcix {namespace} {command} [options]\n</code></pre> <p>Here's an example of the MettleCI Command Line being used to compile an entire DataStage project (using the 'compile' command provided by the 'datastage' namespace) :</p> <pre><code># Note that the example below uses the line continuation charactere ('\\' on Unix or '^' on Windows)\n# to aid readability, but your mettleci command line can all be on a single line if you prefer \n$&gt; mcix datastage compile \\\n   -domain test1-svcs.datamigrators.io:59445 \\\n   -server test1-engn.datamigrators.io -project dstage1 \\\n   -username isadmin -password isadminpwd\nAnalyzing assets to compile\nCompilation folder location = C:\\Apps\\command-shell\\log\\compiliation\nAttempting to compile with 4 working threads.\nCompiling DataStage jobs...\n * Compile 'test2-engn.datamigrators.io/dstage1/Jobs/Load/EX_Account.pjb' - COMPLETED\n [SNIP]\n * Compile 'test2-engn.datamigrators.io/dstage1/Jobs/Load/TX_StockHolding.pjb' - COMPLETED\nCompilation complete\n$&gt; \n</code></pre> <p>Note that MettleCI Command Line namespaces, commands, and options are all case sensitive. </p>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/command-shell/#use-a-password-containing-special-characters","title":"Use a password containing special characters","text":"<p>If the password contains special characters, you will need to wrap it with single or double quote or by using escape characters.</p> Password contains Windows-based Unix-based ! (exclamation) Use password without modification. For example: MyPassword! Wrap password with single quote.  For example: 'MyPassword!' \u201c (double quote) Use escape character . For example: My\u201dPassword Wrap password with single quote.  For example: 'My\"Password' ' (single quote) Wrap password with double quote.  For example: \u201cMy'Password\u201d Wrap password with single quote and use escape character .  For example: 'My'''Password' * (asterisk) Use password without modification. For example: My*Password Wrap password with single quote.  For example: 'My*Password' Wrap password with double quote.  For example: \u201cMy Password\u201d Wrap password with single quote.  For example: 'My Password'","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/command-shell/#using-external-command-files-with-the-mettleci-cli","title":"Using external command files with the MettleCI CLI","text":"<p>MettleCI allows you to define a MettleCI command in a text-based \u2018command file\u2019 and pass the file as parameter to the MettleCI command.  This is accomplished using the '@' command syntax: </p> <pre><code># Here's a typical command file\n$&gt; cat file mycommand.txt\ndatastage\ncompile\n-domain\ntest1-svcs.datamigrators.io:59445\n-username\nisadmin\n-password\nisadminpwd\n-server\ntest1-engn.datamigrators.io\n-project\ndstage1\n\n# ... and here's how to use it\n$&gt; mcix @mycommand.txt\n</code></pre> <p>Note:</p> <ul> <li>Each element of a command file needs to be on an individual line (i.e. separated by your operating system\u2019s newline ASCII character combination)</li> <li>A command file can only contain the definition of a single MettleCI command</li> <li>You can run the MettleCI Command Line with multiple commands by invoking it with individual command files from a shell script with one command per line. E.g.</li> </ul> <pre><code>#!/usr/bin/env bash\nmcix @mycommand1.txt\nmcix @mycommand2.txt\nmcix @mycommand3.txt\n# etc.\n</code></pre>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/compliance-namespace/","title":"Compliance namespace","text":"","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/compliance-namespace/#compliance-list-tags","title":"Compliance List Tags","text":"<p>This command analyses a specified set of Compliance Rules or Asset Queries and reports the tags defined for each.  Output is available in an easy-to-read tabulated for, or as a CSV for downstream processing. When no format optin</p>","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/compliance-namespace/#example","title":"Example","text":"<p>This example shows how to list the tags of a directory of Compliance Rules in both tabulated and CSV formats:</p> <pre><code># ####################################\n# list-tags output in tabulated format\n# ####################################\n$&gt; mcix compliance list-tags -rules ~/Projects/bitbucket.org/compliance-rules -format table\nMettleCI Command Line (build 174)\n(C) 2018-2022 Data Migrators Pty Ltd\ncompliance list-tags (v2.2.x)\nrules configuration discovered\nincluded rule - 'Adjacent Transformers' (PARALLEL_JOB)\nincluded rule - 'Adjacent Transformers' (SERVER_JOB)\n... &lt;SNIP&gt; ...\nincluded rule - 'Transformer With Unreferenced Stage Variable' (SERVER_JOB)\nincluded rule - 'Unique Sort' (PARALLEL_JOB)\n                                       Rule Name  Asset Type                 example  fail-ci  fail-upgrade  functionality  governance  maintainability  performance  portability  security  testability\n================================================  =========================  =======  =======  ============  =============  ==========  ===============  ===========  ===========  ========  ===========\n                           Adjacent Transformers  PARALLEL_JOB               -------  -------  ------------  -------------  ----------  maintainability  -----------  -----------  --------  -----------\n                           Adjacent Transformers  SERVER_JOB                 -------  -------  ------------  -------------  ----------  maintainability  -----------  -----------  --------  -----------\n                                Audit Annotation  PARALLEL_JOB               example  -------  ------------  -------------  ----------  maintainability  -----------  -----------  security  -----------\n                                Audit Annotation  SEQUENCE_JOB               example  -------  ------------  -------------  ----------  maintainability  -----------  -----------  security  -----------\n                                Audit Annotation  SERVER_JOB                 example  -------  ------------  -------------  ----------  maintainability  -----------  -----------  security  -----------\n                                          &lt;SNIP&gt;  &lt;SNIP&gt;                     ...      ...      ...           ...            ...         ...              ...          ...          ...       ...\n               Transformer Uses Abort After Rows  PARALLEL_JOB               -------  -------  ------------  functionality  ----------  ---------------  -----------  -----------  --------  -----------\n               Transformer Uses Abort After Rows  PARALLEL_SHARED_CONTAINER  -------  -------  ------------  functionality  ----------  ---------------  -----------  -----------  --------  -----------\n    Transformer With Unreferenced Stage Variable  PARALLEL_JOB               -------  -------  ------------  functionality  ----------  maintainability  -----------  -----------  --------  -----------\n    Transformer With Unreferenced Stage Variable  SERVER_JOB                 -------  -------  ------------  functionality  ----------  maintainability  -----------  -----------  --------  -----------\n                                     Unique Sort  PARALLEL_JOB               -------  -------  ------------  -------------  ----------  maintainability  -----------  -----------  --------  -----------\n# ##############################\n# list-tags output in CSV format\n# ##############################\n$&gt; mcix compliance list-tags -rules ~/Projects/bitbucket.org/compliance-rules -format csv\nMettleCI Command Line (build 174)\n(C) 2018-2022 Data Migrators Pty Ltd\ncompliance list-tags (v2.2-SNAPSHOT)\nrules configuration discovered\n... &lt;SNIP&gt; ...\nRule Name,Asset Type,example,fail-ci,fail-upgrade,functionality,governance,maintainability,performance,portability,security,testability\nAdjacent Transformers,PARALLEL_JOB,,,,,,maintainability,,,,\nAdjacent Transformers,SERVER_JOB,,,,,,maintainability,,,,\nAudit Annotation,PARALLEL_JOB,example,,,,,maintainability,,,security,\nAudit Annotation,SEQUENCE_JOB,example,,,,,maintainability,,,security,\nAudit Annotation,SERVER_JOB,example,,,,,maintainability,,,security,\n... &lt;SNIP&gt; ...\nTransformer With Unreferenced Stage Variable,PARALLEL_JOB,,,,functionality,,maintainability,,,,\nTransformer With Unreferenced Stage Variable,SERVER_JOB,,,,functionality,,maintainability,,,,\nUnique Sort,PARALLEL_JOB,,,,,,maintainability,,,,\n$&gt;\n</code></pre>","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/compliance-namespace/#compliance-query-command","title":"Compliance Query Command","text":"This command is for running MettleCI Asset Queries <p>If you're looking for the Compliance Rules returned by DataStage flow analysis then see the Compliance Test Command.</p> <p></p> <p>The command line implementation of the Compliance Query functionality exposes the low-level mechanism to produce a report listing the results of the specified Asset Queries.</p>","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/compliance-namespace/#example_1","title":"Example","text":"<p>This example demonstrates how to export  a set of ISX files and run Asset Queries against them. Note that asset paths specification in the export command uses the same wildcard rules as the istool command.</p> <pre><code># ============================== \n# Export the required ISX assets\n# ============================== \nC:\\&gt; mcix isx export ^\n     -domain myteam-svcs.corp.com:59445 ^\n     -username myuser -password mypassword ^\n     -server myteam-engn.corp.com ^\n     -project myproject ^\n     -jobname .*LD_S.*\nExporting [.*LD_S.*] from repository...\nExporting DataStage assets...\n * Export 'test2-engn.datamigrators.io/myproject/Jobs/Load/LD_SUPPLIER.pjb' - COMPLETED\n * Export 'test2-engn.datamigrators.io/myproject/Jobs/Load/LD_STOCK_HOLDING.pjb' - COMPLETED\n * Export 'test2-engn.datamigrators.io/myproject/Jobs/Load/LD_STOCKITEM.pjb' - COMPLETED\n * Export 'test2-engn.datamigrators.io/myproject/Jobs/Load/LD_SALE.pjb' - COMPLETED\nExport complete\n# ================================================================\n# Run the specified asset queries against the exported ISX assets\n# ================================================================\nC:\\&gt; mcix compliance query \\\n     -assets ./Jobs \\\n     -queries ./Queries \\\n     -report compliance.csv \\\nMettleCI Command Line (build 122)\n(C) 2018-2020 Data Migrators Pty Ltd\n &lt;SNIP&gt;\n# Done!\nC:\\&gt;\n</code></pre>","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/compliance-namespace/#compliance-test-command","title":"Compliance Test Command","text":"This command is for running MettleCI Compliance Rules <p>If you're looking for the Asset Queries typically used in a MettleCI Report Card then please see the Compliance Query Command.</p> <p></p> <p>The command line implementation of the Compliance Test functionality enables the production of a Compliance Results report of the specified assets against the specified set of MettleCI Compliance Rules.</p> <p>For more information on using the <code>-project-cache</code> parameter see our detailed explanation.</p>","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/compliance-namespace/#example_2","title":"Example","text":"<p>This example demonstrates how to export  a set of ISX files and run Compliance against them. Note that asset paths specification in the export command uses the same wildcard rules as the istool command.</p> <pre><code># ============================== \n# Export the required ISX assets\n# ============================== \nC:\\MettleCI\\cli\\&gt; mcix isx export ^\n     -domain myteam-svcs.corp.com:59445 ^\n     -username myuser -password mypassword  ^\n     -server myteam-engn.corp.com  ^\n     -project myproject  ^\n     -location C:\\shared\\myproject\\export  ^\n     -include-binaries  ^\n     -project-cache C:\\shared\\myproject\\cache\nAnalyzing test2-engn.datamigrators.io/myproject\nAttempting to identify changes with 4 working threads.\nInspecting DataStage assets for changes...\n&lt;SNIP&gt;\nChange identification complete\nInspecting ParameterSet definition changes...\nParameterSet definition change identification complete\nDeleting assets...\n&lt;SNIP&gt;\nDeletion complete\nExporting DataStage assets...\n&lt;SNIP&gt;\nExport complete\nAttempting to identify last change with 4 working threads.\nInspecting DataStage assets for last change...\n&lt;SNIP&gt;\nLast change identification complete\n# ==================================================================\n# Run the specified compliance rules against the exported ISX assets\n# ==================================================================\n$&gt; mcix compliance test\n  -rules compliance_rules\n  -assets datastage\n  -report compliance_report_warn.xml\n  -junit\n  -project-cache ./project-cache\n  -test-suite warnings\n  -ignore-test-failures\n  -include-job-in-test-name\nMettleCI Command Line (build 122)\n(C) 2018-2020 Data Migrators Pty Ltd\nrules configuration discovered\nnew rule discovered - 'Adjacent Transformers' (PARALLEL_JOB)\nnew rule discovered - 'CCMigrateTool Stages' (PARALLEL_JOB)\nnew rule discovered - 'CCMigrateTool Stages' (SERVER_JOB)\nnew rule discovered - 'Database Row Limit' (PARALLEL_JOB)\nnew rule discovered - 'Database Row Limit' (SERVER_JOB)\nnew rule discovered - 'Debug Row Limit' (PARALLEL_JOB)\n&lt;SNIP&gt;\nnew rule discovered - 'One Dataflow' (SERVER_JOB)\nnew rule discovered - 'Range Lookup' (PARALLEL_JOB)\nnew rule discovered - 'Too Many Stages' (PARALLEL_JOB)\nnew rule discovered - 'Too Many Stages' (SERVER_JOB)\nnew rule discovered - 'Unique Sort' (PARALLEL_JOB)\n[1/3] TestJob_0921 (PARALLEL_JOB)\n[2/3] TestJob_0930 (PARALLEL_JOB)\n[3/3] TestJob (PARALLEL_JOB)\n# Done!\n$&gt;\n</code></pre>","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/compliance-namespace/#references","title":"References","text":"<p>For a discussion on the use of the <code>include-tags</code> and <code>exclude-tags</code> options see Compliance Rule Tags.</p>","tags":["Pipeline","CLI","Flow Analysis","Tags"]},{"location":"mettleci-cli/datastage-namespace/","title":"DataStage namespace","text":"","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#datastage-capture-command","title":"DataStage Capture Command","text":"<p>This command captures the current state of a DataStage project by exporting its content to ISX files and updating the project cache.  The resulting ISX files and project cache can be used by the DataStage Deploy command to incrementally deploy changes that have occurred after DataStage Capture was performed.</p> <ul> <li>the <code>-location</code> parameter specifies the root directory of ISX files exported from the project.</li> <li>the <code>-project</code> value is the name of the DataStage target project.</li> <li>the <code>-project-cache</code> parameter specifies a shared directory containing state information for this DataStage target project.  These are the DataStage asset fingerprints which are used to identify changes in your DataStage code.  See a detailed explanation.</li> <li>the <code>-threads</code> parameter specifies how many concurrent compilation operations will be performed.</li> </ul>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#example","title":"Example","text":"<p>Capture the current state of the project. </p> <pre><code>c:\\&gt; mcix datastage capture ^\n     -domain datastage-services.myorganization.com:59445 ^\n     -server datastage-engine.myorganization.com ^\n     -project myproject ^\n     -username isadmin ^\n     -password **** ^\n     -location datastage ^\n     -threads 4 ^\n     -project-cache \"C:\\MettleCI\\cache\\datastage-engine.myorganization.com\\myproject\" \nMettleCI Command Line (build 128)\n(C) 2018-2022 Data Migrators Pty Ltd\nAnalyzing DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject\nExporting DataStage assets...\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Load/LD_SUPPLIER.pjb' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Connections/DMSqlServer_OLTP.dcn' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Utilities/UT_CONTROL.pjb' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Load/LD_TRANSACTION.pjb' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Extract/EX_PURCHASE.pjb' - COMPLETED\n&lt;snip&gt;\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Transform/TR_PURCHASE.pjb' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Transform/TR_PAYMENT_METHOD.pjb' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Extract/EX_SUPPLIER.pjb' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Transform/TR_STOCK_HOLDING.pjb' - COMPLETED\n * Export 'DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject/Jobs/Extract/EX_EMPLOYEE.pjb' - COMPLETED\nExport complete\nAttempting to identify last change with 4 working threads.\nInspecting DataStage assets for last change...\nLast change to project occurred at 2023-12-21 04:37:37 GMT \n</code></pre> <p>The DataStage Project has been captured as a set of ISX files and the project cache.  Deploying the captured state back into the source project will not cause any changes to be detected:</p> <pre><code>c:\\&gt; mcix datastage deploy ^\n     -domain datastage-services.myorganization.com:59445 ^\n     -server datastage-engine.myorganization.com ^\n     -project myproject ^\n     -username isadmin ^\n     -password **** ^\n     -assets datastage ^\n     -threads 4 ^\n     -project-cache \"C:\\MettleCI\\cache\\datastage-engine.myorganization.com\\myproject\" \nMettleCI Command Line (build 128)\n(C) 2018-2022 Data Migrators Pty Ltd\nAnalyzing DATASTAGE-ENGINE.MYORGANIZATION.COM/myproject\nAttempting to identify changes with 4 working threads.\nInspecting DataStage assets for changes...\nChange identification complete, 0 asset changes detected in project\n</code></pre>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#datastage-cleanup-projects","title":"DataStage Cleanup-Projects","text":"<p>Deletes redundant DataStage projects matching a supplied pattern.</p>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#example_1","title":"Example","text":"<pre><code>$&gt; mcix datastage cleanup-projects \\\n   -domain my-services.datamigrators.io:59445 \\\n   -username isadmin \\\n   -password isadminpwd \\\n   -server my-engine.datamigrators.io \\\n   -pattern Test[0-9] \\\n   -retain 1\nListing projects:\n  - ANALYZERPROJECT\n  - DataClick\n  - dstage1\n  - Test1\n    - matches pattern\n  - Test2\n    - matches pattern\n  - Test4\n    - matches pattern\n  - SWPensionStrategy\n  - wwi_prod\nCleaning up old projects, retaining 1 most recent projects\n * Delete 'test2-engn.datamigrators.io/Test4' - SKIPPED\nDeleting project: SNTest2\n * Delete 'test2-engn.datamigrators.io/Test2' - COMPLETED\nDeleting project: SNTest1\n * Delete 'test2-engn.datamigrators.io/Test1' - COMPLETED\n</code></pre>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#datastage-compile","title":"DataStage Compile","text":"<p>Compiles a DataStage Job producing a jUnit-compatible testing output that can be utilised by built tools orchestrating a CI/CD pipeline.  This command produces a JUnit-compatible XML file called <code>mettleci_compilation.xml</code> which reports each individual job\u2019s compilation result.</p>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#example_2","title":"Example","text":"<pre><code>$&gt; mcix datastage compile \\\n   -domain service_tier.datamigrators.io:59445 \\\n   -username isadmin \\\n   -password mypassword \\\n   -server engine_tier.datamigrators.io \\\n   -project dstage1 \\\n   -include-job-in-test-name\nAnalyzing assets to compile\nCompilation folder location = C:\\Apps\\command-shell\\log\\compiliation\nAttempting to compile with 4 working threads.\nCompiling DataStage jobs...\n * Compile 'engine_tier.datamigrators.io/dstage1/Jobs/Load/LD_STOCKITEM.pjb' - COMPLETED\n * Compile 'engine_tier.datamigrators.io/dstage1/Jobs/Load/LD_SALE.pjb' - COMPLETED\n * Compile 'engine_tier.datamigrators.io/dstage1/Jobs/Load/LD_SUPPLIER.pjb' - COMPLETED\n * Compile 'engine_tier.datamigrators.io/dstage1/Jobs/Load/LD_STOCK_HOLDING.pjb' - COMPLETED\nCompilation complete\n</code></pre>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#datastage-create-project","title":"DataStage Create-Project","text":"<p>This command either creates a DataStage project in a nominated environment or simply exist with a success code if the DataStage project already exists. It is used frequently at the beginning of pipelines to assert that a target environment with which the pipeline will deploy and execute code is present and available.</p>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#example_3","title":"Example","text":"<pre><code>$&gt; mcix datastage create-project \\\n   -domain service_tier.datamigrators.io:59445 \\\n   -username isadmin -password mypassword \\\n   -server engine_tier.datamigrators.io \\\n   -project Test4\nTest4 created successfully.\n$&gt;\n</code></pre>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#usage-notes","title":"Usage Notes","text":"<p>Due to a known issue with the DataStage dsadmin command itself it is not possible to distinguish between\u2026</p> <ul> <li>a DataStage project that already exists, and</li> <li>a DataStage project that doesn\u2019t exist in the DataStage repository, but for which the associated filesystem directories does exist.</li> </ul> <p>There may be some situations in which this causes the <code>mettleci datastage create-project</code> command to fail. When faced with an inexplicable failure of this nature check to see if the project\u2019s directory structure already exists on the filesystem.  If so, and it\u2019s safe to do so, remove the file structure and try again.</p>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#datastage-delete-project","title":"DataStage Delete-Project","text":"<p>Deletes a DataStage project.</p>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#example_4","title":"Example","text":"<pre><code>$&gt; mcix datastage delete-project \\\n   -domain test2-svcs.datamigrators.io:59445 -server test2-engn.datamigrators.io \\\n   -username isadmin -password isadminpwd \\\n   -project Test3\n</code></pre>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#datastage-deploy","title":"DataStage Deploy","text":"<p>This command deploys a specified directory containing one or more DataStage ISX files to a specified target DataStage environment (project). </p> <ul> <li>the datastage deploy command performs incremental deployment.</li> <li>the <code>-assets</code> parameter specifies the deployment source directory containing ISX files.</li> <li>the <code>-project</code> value is the name of the DataStage target project.</li> <li>the <code>-project-cache</code> parameter specifies a shared directory containing state information for this DataStage target project.  These are the DataStage asset fingerprints which are used to identify changes in your DataStage code.  See our more detailed explanation.</li> <li>the <code>-threads</code> parameter specifies how many concurrent compilation operations will be performed.</li> <li>DataStage compilation results are converted to test results in JUnit format.</li> </ul> <p>See Repeatable DataStage Project Deployments for more details on how the <code>-project-cache</code> parameter is used to implement incremental deployment.</p>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#example_5","title":"Example","text":"<pre><code>c:\\&gt; mcix datastage deploy \\\n     -domain datastage-services.myorganization.com:59445 \\\n     -server datastage-engine.myorganization.com \\\n     -project wwi_jenkins_ds_115_ci \\\n     -username isadmin \\\n     -password **** \\\n     -assets datastage \\\n     -parameter-sets \"config\\Parameter Sets\" \\\n     -threads 1 \\\n     -project-cache \"C:\\MettleCI\\cache\\datastage-engine.myorganization.com\\jenkins_project\" \nMettleCI Command Line (build 128)\n(C) 2018-2022 Data Migrators Pty Ltd\nAnalyzing datastage-engine.myorganization.com/myproject\nAttempting to identify changes with 4 working threads.\nInspecting DataStage assets for changes...\n* Check datastage-engine.myorganization.com/myproject/Jobs/Transform/TR_CITY.pjb - CHANGED\nChange identification complete, 1 asset changes detected in project\nDeleting assets...\n* Delete 'datastage-engine.myorganization.com/myproject/Jobs/Transform/TR_CITY.pjb' - COMPLETED\nDeletion complete\nOptimising assets for import\n* Update 'Jobs/ParameterSets/pGlobal.pst' - COMPLETED\n* Update 'Jobs/ParameterSets/pDMSqlServer_DW.pst' - COMPLETED\ns* Update 'Jobs/ParameterSets/pDMSqlServer_OLTP.pst' - COMPLETED\nAttempting to import with 1 working threads.\nImporting DataStage assets...\n* Import 'datastage-engine.myorganization.com/myproject/Jobs/ParameterSets/pDMSqlServer_OLTP.pst' - COMPLETED\n* Import 'datastage-engine.myorganization.com/myproject/Jobs/ParameterSets/pDMSqlServer_DW.pst' - COMPLETED\n* Import 'datastage-engine.myorganization.com/myproject/Jobs/ParameterSets/pGlobal.pst' - COMPLETED\n* Import 'datastage-engine.myorganization.com/myproject/Jobs/Transform/TR_CITY.pjb' - COMPLETED\nImport complete\nCompiling DataStage jobs...\n* Compile 'datastage-engine.myorganization.com/myproject/Jobs/Transform/TR_CITY.pjb' - COMPLETED\nCompilation complete\nsCreating JUnit test suite\nJUnit test suite (mettleci_compilation.xml) created successfully\nAttempting to identify last change with 4 working threads.\nInspecting DataStage assets for last change...\nLast change to project occurred at 2023-12-17 22:07:00 GMT     \n</code></pre>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#datastage-execute","title":"DataStage Execute","text":"<p>Execute a DataStage job.</p>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/datastage-namespace/#example_6","title":"Example","text":"<pre><code>$&gt; mcix datastage execute \\\n   -domain test1-svcs.datamigrators.io:59445 \\\n   -server test1-engn.datamigrators.io \\\n   -username isadmin -password password1 \\\n   -project dstage1 \\\n   -jobname TR_ORDERS \\\n   -runmode NORMAL\n</code></pre>","tags":["CLI","Pipelines","Tags"]},{"location":"mettleci-cli/fabrication-namespace/","title":"Fabrication namespace","text":"","tags":["DataStage","Data Fabrication"]},{"location":"mettleci-cli/fabrication-namespace/#fabriction-list","title":"Fabriction List","text":"","tags":["DataStage","Data Fabrication"]},{"location":"mettleci-cli/fabrication-namespace/#fabrication-test","title":"Fabrication Test","text":"<p>Here</p>","tags":["DataStage","Data Fabrication"]},{"location":"mettleci-cli/project-cache-directory/","title":"The Project Cache Directory","text":"<p>The <code>-project-cache</code> parameter, which refers to a filesystem directory, is used with some MettleCI CLI commands to enable incremental operations. The directory supplied to the <code>-project-cache</code> option is the location where the CLI will read/write state information (sometimes referred to as asset fingerprints) used for performing incremental operations.  This directory should exist wherever the MettleCI CLI executes a command which relies on incremental behaviour, which normally occurs on the MettleCI Agent host under the instruction of your build agent.</p> <p>In the sample pipelines shipped with MettleCI the incremental MettleCI CLI commands, assume the use of a locally-stored project cache and refer to the following project cache location:</p> <pre><code>%AGENTMETTLEHOME%\\cache\\%IISENGINENAME%\\%DATASTAGE_PROJECT%\n</code></pre> <p>\u2026 which will normally translate to something similar to \u2026</p> <pre><code>C:\\MettleCI\\CLI\\cache\\MY.ENGINE.HOSTNAME\\MyDataStageProject\\\n</code></pre> Note <p>The project cache will always be a Windows-style filesystem reference (using backslashes), as many of the MettleCI CLI commands required in a CI/CD pipeline for pre-NextGen DataStage rely on Windows-only DataStage Client utilities.</p> <p>There are a few things which need to be considered when deciding the location of project cache directories:</p> <ul> <li> <p>The directory must be unique to a DataStage project</p> </li> <li> <p>It must be directly accessible from the CLI (i.e.. You can't specify an engine path if the CLI is running on a Client)</p> </li> <li> <p>ADVANCED: If multiple instances of the CLI are to be used for incremental operations (and hence multiple independent CLI instances need to share a common view of the incremental environment\u2019s status) then the project cache needs to be available and synchronised across all of those CLI instances.  This is normally achieved using shared storage.</p> </li> </ul>","tags":["CLI"]},{"location":"mettleci-cli/project-cache-directory/#using-multiple-cli-environments","title":"Using multiple CLI environments","text":"<p>The last point becomes important if you are running a \u2018pool\u2019 of agents on the CI/CD pipeline.  If they were to all maintain their own independent copies of the project cache files then incremental fingerprints will differ between CLI instances, resulting in a significant drop in the performance benefits delivered by the incremental approach.</p> <p>As an example of what could happen if a mettleci datastage deploy  command deploys to the same DataStage project with two different -project-cache directories:</p> <ol> <li>Initial Deployment using MyCache<ol> <li>Command <code>mcix datastage deploy ... -project-cache MyCache</code> is invoked.</li> <li>This imports and compiles all ISX files as MyCache does not currently contain any stage information.</li> <li>This initially expensive process will always be required to establish. the initial contents of the project cache.</li> </ol> </li> <li>First Deployed Change using OtherCache<ol> <li>Job <code>MyFirstJob</code> is checked in.</li> <li>Command <code>mcix datastage deploy ... -project-cache OtherCache</code> is invoked.</li> <li>This imports and compiles all ISX files as OtherCache does not currently contain any stage information.</li> </ol> </li> <li>Second Deployed Change using MyCache<ol> <li>Command <code>mcix datastage deploy ... -project-cache MyCache</code> is run.</li> <li>This compares the Git codebase to the target DataStage environment and sees that all fingerprints are misaligned.  The command consequently re-imports and re-compiles all ISX files as MyCache contains state information which is not aligned with the target.  This could have been avoided using the proper project cache.</li> </ol> </li> </ol> <p>Alternatively you can use you agent labelling strategy to ensure that only a single CLI instance is used by your pipeline which eliminates the need to coordinate the project-cache amongst multiple environments.  In these cases a locally-stored project cache can be used.  If you subsequently decide to horizontally scale your agent capability then you will need to\u2026</p> <ol> <li>Move your project cache to shared storage, and </li> <li>Modify your build pipelines to refer to that new shared location.</li> </ol>","tags":["CLI"]},{"location":"mettleci-cli/unittest-namespace/","title":"UnitTest namespace","text":"","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/unittest-namespace/#unittest-generate","title":"UnitTest Generate","text":"<p>Generates a DataStage test case for one or more specified DataStage flows.</p> <p>The optional <code>-check-row-count-only</code> flag will cause the generation of a test case which checks row counts, rather than the default option which is to compare data row-by-row.</p>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/unittest-namespace/#example","title":"Example","text":"<pre><code>$&gt; mcix unittest generate \\\n   -assets /opt/dm/mci/jobs \\\n   -joblist ./joblist.txt \\\n   -specs /opt/dm/mci/testspecs\n</code></pre>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/unittest-namespace/#unittest-test","title":"UnitTest Test","text":"<p>Run one or more MettleCI Unit Tests against one or more DataStage jobs.</p> <p>The <code>-reports</code> option is used to specify the directory into which the JUnit XML files produced by this command will be placed.  Each job tested will produce a separate XML file named after the Job (e.g. Job MY_JOB_ABC will produce a JUnit file named  MY_JOB_ABC.xml)</p> <p>The <code>-ignore-test-failures</code> option will prevent a failing Unit Test from being interpreted as a command failure by your build system, and consequently halting your CI/CD pipeline. </p> <p>See Repeatable DataStage Project Deployments for more details on how the -project-cache parameter is used to implement incremental tests. For more information on using the <code>-project-cache</code> parameter see our detailed explanation.</p>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/unittest-namespace/#the-ignore-test-failures-option","title":"The 'ignore-test-failures' option","text":"<p>MettleCI unit tests can be executed in two ways: </p> <ol> <li>Manually, using the MettleCI Workbench tool\u2019s under interface, or</li> <li>From the command line using the mettleci unittest test (documentation) command.</li> </ol> <p>When using the MettleCI command line to execute unit tests from within a build orchestration system (Jenkins, GitHub actions, Bamboo, etc.) it\u2019s important to understand how the mettleci unittest test command and your build system interact.</p> <p>The  mettleci unittest test command has three potential outcomes:</p> <ul> <li>The command executes successfully and runs a unit test which passes,</li> <li>The command executes successfully and runs a unit test which fails, or</li> <li>The command fails to execute for any reason, and the unit test is never invoked (e.g. due to a misconfigured parameter such as referencing a non-existent unit test)</li> </ul> <p>Like all shell commands, the <code>mettleci unittest test</code> command returns an exit code informing the host system of the success, or otherwise, of the invoked process - in this case a DataStage test case. By default, the <code>mettleci unittest test</code> command returns a non-zero (failure) result when either the command cannot complete or when a unit test fails. For many build orchestration systems this will cause the build to fail instantly and, most importantly, prevent the publication of the failed test\u2019s associated JUnit XML file, making the process of diagnosing the test failure difficult.</p> <p>The <code>mettleci unittest test -ignore-test-failures</code> option will prevent a failing unit test from being interpreted as a command failure by your build system, and consequently halting your CI/CD pipeline.</p>","tags":["DataStage","Running Tests"]},{"location":"mettleci-cli/unittest-namespace/#example_1","title":"Example","text":"<pre><code>C:\\&gt; mcix unittest test ^\n    -domain test1-svcs.datamigrators.io:59445 ^\n    -server test1-engn.datamigrators.io ^\n    -username isadmin ^\n    -password my_password ^\n    -project my_project ^\n    -specs unittest ^\n    -reports unittest_reports ^\n    -project-cache \"C:\\MettleCI\\cache\\test1-engn.datamigrators.io\\my_project\"\nMettleCI Command Line (build 128)\n(C) 2018-2022 Data Migrators Pty Ltd\nLoading Unit Test Specifications from 'unittest'\nReading test1-engn.datamigrators.io/my_project\nAttempting to identify changes with 1 working threads.\nInspecting DataStage assets for changes...\n * Check test1-engn.datamigrators.io/my_project/Jobs/Transform/TR_ORDERS.pjb - COMPLETED\nChange identification complete\nExecuting Tests with 4 concurrent jobs...\n * Test TR_ORDERS/TR_ORDERS - SKIPPED\nUpdating incremental state...\nAttempting to identify last change with 1 working threads.\nInspecting DataStage assets for last change...\n * Check test1-engn.datamigrators.io/my_project/Jobs/Transform/TR_ORDERS.pjb - COMPLETED\nLast change identification complete\nTest execution completed successfully.\nC:\\&gt;\n</code></pre>","tags":["DataStage","Running Tests"]},{"location":"overview/overview/","title":"Overview","text":"<p>This page will present an introduction to the testing pyramid and some testing concepts outline superbly by Martin Fowler on his site.</p> <p></p> <p>\u2026 and we'll introduce 'shift left' testing philosophy outlined by IBM here.</p>","tags":["Pipelines","CLI","Deploy","Build"]},{"location":"overview/overview/#_1","title":"Overview","text":"","tags":["Pipelines","CLI","Deploy","Build"]},{"location":"patterns/high-volume-tests/","title":"High Volume DataStage Tests","text":"<p>During the execution of a DataStage\u00ae test case the data produced by a job (on one or more output links) is compared against expected test data to identify and report on any differences.  When testing with large volumes of data, the comparison process may consume too much memory and cause your test job to abort with a fatal error.  The simplest approach to resolving this issue is to reduce your test data volume to the smallest number of records necessary to exercise each code path through your flow.  Doing so will ensure that your test cases execute quickly and can be easily understood and maintained.</p> <p>In the event that test data available to you cannot easily be reduced, the memory required by the data comparison process can be reduced by specifying a Cluster Key in the test specification.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/high-volume-tests/#using-cluster-keys-in-datastage-tests","title":"Using Cluster Keys in DataStage Tests","text":"<p>Defining a Cluster Key will cause DataStage to split the actual data output and expected data into multiple, smaller subsets before the data is compared.  Data is split such that each subset will only contain records that have the same values for all columns that make up the Cluster Key - a process somewhat analogous to DataStage partitioning.  The data are then sorted and a comparison of actual and expected data is performed using multiple, smaller operations which require less memory and are performed sequentially.  </p> <p>Test result behavior: Due to the iterative nature of comparisons using a Cluster Key, each record which has differences in the Cluster Key columns will be reported as 1 added record and 1 removed record rather than shown as a single record with a change indicator.</p> <p>A good Cluster Key is one that results in data subsets which strike a balance between the following factors:</p> <ul> <li>Each subset should fit in memory during comparison. Test execution will abort when memory thresholds are breached.</li> <li>Are as large as possible given the memory constraint. Lots of tiny subsets will degrade comparison performance.</li> </ul> <p>Selecting an appropriate Cluster Key might require several iterations to find a column (or combination of columns) which not only prevents Job aborts but also keeps run times acceptable.  Unless you are comparing unusually wide records, a good starting point is to aim for each subset of data to contain no more than 1,000 records and adjust the Cluster Key if memory thresholds continue to be breached.</p> <p>If you have used Interception to capture some input and / or expected test data for a DataStage Test and subsequently decide you want to apply a Cluster Key, you don\u2019t have to re-run Interception. This is also the case if you\u2019ve manually created any test data files. The Cluster Key is used at run time and therefore doesn\u2019t require any additional data preparation by the user.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/high-volume-tests/#example","title":"Example","text":"<p>Consider the situation where a DataStage Test has to compare several million financial transaction records with the following schema on the 'order_out' link of stage 'ODBC_order':</p> Column name SQL type Length Scale Nullable Transaction_Date Timestamp No Account_Id VarChar 20 No Type_Code VarChar 5 No Description VarChar Yes Amount Decimal 18 2 No <p>The test specification can be updated with a Cluster Key to enable iterative comparison of actual and expected test data.  In this example, <code>Account_Id</code> and <code>Type_Code</code> are defined as the compound Cluster Key:</p> <pre><code>{\n    \"then\": [\n        {\n            \"path\": \"ODBC_orders.csv\",\n            \"stage\": \"ODBC_order\",\n            \"link\": \"order_out\",\n            \"cluster\": [\n                \"Account_Id\",\n                \"Type_Code\"\n            ]\n        }\n    ],\n}\n</code></pre> <p>Note: Cluster Keys are specified on a per-link basis. DataStage flows with multiple output links can use any combination of clustered and non-clustered comparisons within a single test specification.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/high-volume-tests/#caveats","title":"Caveats","text":"<p>Cluster keys should be chosen to break Actual and Expected data into clusters which are small enough to fit in memory.</p> <p>Note that if a Unit Test detects a value difference in a column which is a cluster key column, then the Unit Test difference report (which would normally describe the difference as a \u2018modified\u2019 row when not using a cluster key) will now describe the difference as distinct \u2018added\u2019 and \u2018removed\u2019 entries.  </p> <p>As useful as Cluster Keys are, it\u2019s poor practice to simply apply them to every DataStage test that has to process high data volumes. You will almost certainly find combinations of flows and data volumes in your project where no Cluster Key will reduce the memory demands of a DataStage test enough to avoid Job aborts (See Unit Test throws OutOfMemoryError exception). In these situations you can manage your test data volumes by \u2026</p> <ul> <li>carefully selecting a subset of records from your data sources,</li> <li>using the DataStage's data fabrication features, or</li> <li>both of these approaches in combination.</li> </ul>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/","title":"Testing flows with rejects","text":"<p>Most DataStage jobs can be tested via MettleCI\u2019s Unit Testing function simply by replacing input and output stages. However, some job designs - while commonplace - will necessitate a more advanced Unit Testing configuration.  The sections below outline MettleCI Unit Test Spec patterns that best match these job designs.</p> This patterns is handled automatically for you by DataStage <p>For DataStage flows that use this pattern the DataStage test case creation process will generate an appropriately structured test case specification. This page acts as a reference to explain the structure of the generated JSON test specification.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#input-stage-with-rejects","title":"Input stage with rejects","text":"<p>The Input stage can be Unit Tested by including both read and reject links in the given clause of the Unit Test Spec.</p> <p>The CSV data specified for the rejects link should contain records that will actually test the flow of records through the reject path(s) of the job.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#flow-design","title":"Flow design","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#test-specification","title":"Test specification","text":"<pre><code>{\n   \"given\": [\n      {\n         \"stage\": \"Input\",\n         \"link\": \"Read\" \n         \"path\": \"Input-Read.csv\",\n      },\n      {\n         \"stage\": \"Input\",\n         \"link\": \"Rejects\" \n         \"path\": \"Input-Rejects.csv\",\n      }\n   ],\n   \"when\": {\n      \"parameters\": { }\n   },\n   \"then\": [\n   ]\n}\n</code></pre>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#result","title":"Result","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#output-stage-with-rejects","title":"Output stage with rejects","text":"<p>The output stage can be Unit Tested by including:</p> <ul> <li>the write link in the then clause of the Unit Test Spec; and</li> <li>the reject in the given clause of the Unit Test Spec.</li> </ul> <p>The CSV data specified for the rejects link should contain records that will actually test the flow of records through the reject path(s) of the job.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#flow-design_1","title":"Flow design","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#test-specification_1","title":"Test specification","text":"<pre><code>{\n   \"given\": [\n      {\n         \"stage\": \"Output\",\n         \"link\": \"Rejects\",\n         \"path\": \"Output-Rejects.csv\"\n      }\n   ],\n   \"when\": {\n      \"parameters\": { }\n   },\n   \"then\": [\n      {\n         \"stage\": \"Output\",\n         \"link\": \"Write\",\n         \"path\": \"Output-Write.csv\"\n      }\n   ]\n}\n</code></pre>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-flow-rejects/#result_1","title":"Result","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-stored-procedures/","title":"Testing flows with stored procedure stages","text":"<p>A connector stage which supports stored procedures will not only connect to an external Database for processing but it will also produce output records which are not deterministic.  A DataStage test case specification needs to treat stored procedure-capable stages which will be stubbed during unit test execution.  This is done by adding the relevant stage's input link to the <code>then</code> clause of the test specification and the output link to the <code>given</code> clause.</p> <p>The CSV input specified by the given clause contains the data that will become the flow of records from the Stored Procedure stage. The data could simulate what would be produced by the real stored procedure if it had processed the Unit Test input records, however they don\u2019t have to.</p> <p>[!NOTE] This process is handled automatically by the DataStage test case creation process. This page acts as a reference to explain the structure of the JSON test specification generated for flows including following this pattern.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-stored-procedures/#flow-design","title":"Flow design","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-stored-procedures/#test-specification","title":"Test specification","text":"<pre><code>{\n    \"given\": [\n        {\n            \"path\": \"StoredProcedure-Output.csv\",\n            \"stage\": \"StoredProcedure\",\n            \"link\": \"Output\" \n        }\n    ],\n    \"then\": [\n        {\n            \"path\": \"StoredProcedure-Input.csv\",\n            \"stage\": \"StoredProcedure\",\n            \"link\": \"Input\"\n        }\n    ],\n    \"when\": {\n        \"parameters\": { }\n    }\n}\n</code></pre>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-stored-procedures/#result","title":"Result","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-subflows/","title":"Testing flow with sub-flows","text":"This patterns is handled automatically for you by DataStage <p>For DataStage flows that use this pattern the DataStage test case creation process will generate an appropriately structured test case specification. This page acts as a reference to explain the structure of the generated JSON test specification.</p> <p>Let's take a simple example of a DataStage test case:</p> <p></p> <p>Subflows, both local and components, complicate this situation as stage names in DataStage are only unique within a given flow or container.  </p> <p>Consider writing a test specification for the following flow, <code>ProcessAccounts</code>, which includes a shared subflow stage <code>ContainerC1</code> which is itself a reference to the shared subflow <code>scWriteAccounts</code>:</p> <p>Flow <code>ProcessAccounts</code>: </p> <p>Container <code>scWriteAccounts</code>:  |</p> <p>You might be tempted to create a spec like this \u2026</p> <pre><code>{\n    \"given\": [\n        {\n            \"path\": \"given.csv\",\n            \"stage\": \"sfInput\",\n            \"link\": \"in\" \n        }\n    ],\n    \"when\": {},\n    \"then\": [\n        {\n            \"path\": \"expected.csv\",\n            \"stage\": \"sfOutput\",\n            \"link\": \"out\"\n        }\n    ]\n}\n</code></pre> <p>The resulting Unit Test Spec is ambiguous because the MettleCI Unit Test Harness will not be able to uniquely identify which Unit Test Data file is associated with each sqAccounts stage.  To avoid these sort of issues, the stage properties within Unit Test Specs expect fully qualified stage names.  A fully qualified stage name is prefixed with any relevant parent Container names using the format <code>&lt;container name&gt;.&lt;stage name&gt;</code>.</p> <p>Here\u2019s an example of a fully qualified stage name:</p> <pre><code>{\n    \"given\": [\n        {\n            \"path\": \"given.csv\",\n            \"stage\": \"sfInput\",\n            \"link\": \"in\" \n        }\n    ],\n    \"when\": {},\n    \"then\": [\n        {\n            \"path\": \"expected.csv\",\n            \"stage\": \"subflow_1.sfOutput\",\n            \"link\": \"out\"\n        }\n    ]\n}\n</code></pre>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-surrogate-key-generators/","title":"Testing flows with surrogate key generator stages","text":"<p>Surrogate Key Generators are backed by a Database or a Flat File and will produce output records which are not deterministic.  The use of a Database-backed Surrogate Key Generator will also require a live connection to an external Database which is not ideal for unit testing.  To unit test flow designs containing this type of surrogate key generator, the surrogate key generator stage needs to be removed from the flow and replaced with test case data.  This is done by adding the input link in the <code>then</code> clause of the test specification and the output link in the <code>given</code> clause.</p> <p>The CSV input specified by the given clause contains the data that will become the flow of records from the surrogate key generator stage.  The data could simulate what would be produced by the real surrogate key generator if it had processed the test case input records, however it doesn\u2019t have to.  The easiest way to simulate the surrogate key generator output records would be to copy the CSV specified in the <code>then</code> clause and add a new column to represent the generated key and set appropriate key values.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-surrogate-key-generators/#flow-design","title":"Flow design","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-surrogate-key-generators/#test-specification","title":"Test specification","text":"<pre><code>{\n   \"given\": [\n      {\n         \"stage\": \"KeyGenerator\",\n         \"link\": \"Output\",\n         \"path\": \"KeyGenerator-Output.csv\"\n      }\n   ],\n   \"when\": {\n      \"parameters\": {\n         \"MyKeyStartValue\": 100\n      }\n   },\n   \"then\": [\n      {\n         \"stage\": \"KeyGenerator\",\n         \"link\": \"Input\",\n         \"path\": \"KeyGenerator-Input.csv\"\n      }\n   ]\n}\n</code></pre>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-datastage-surrogate-key-generators/#result","title":"Result","text":"","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-flows-using-datetime-references/","title":"Testing flows using date/time references","text":"<p>Repeatable DataStage tests require that the flow being tested produces deterministic data based on a set of predefined inputs and parameters.  Calculations based on the current date or time are common in DataStage flow designs but will cause the output produced by those flows to change depending on the date and time when its job is executed.  This page outlines practices aimed at ensuring jobs using current date calculations are able to be validly tested.</p>","tags":["DataStage","Creating Tests"]},{"location":"patterns/testing-flows-using-datetime-references/#transformer-stages-using-system-time-and-date-functions","title":"Transformer Stages using system time and date functions","text":"<p>Transformer stages with derivations using the <code>CurrentDate()</code>, <code>CurrentTime()</code> or <code>CurrentTimestamp()</code> functions need to be considered carefully whn designing your tests. Unless your calculation requires a specific date and/or time then calls to the standard <code>CurrentDate()</code>, <code>CurrentTime()</code> and <code>CurrentTimestamp()</code> functions could potentially be substituted with calls to the <code>DSJobStartDate</code>, <code>DSJobStartTime</code> and <code>DSJobStartTimestamp</code> macros. This enables you to substitute them with a specific value during testing.  Add <code>DSJobStartDate</code>, <code>DSJobStartTime</code>, and <code>DSJobStartTimestamp</code> (as required) to the <code>parameters</code> clause of the test Specification's <code>when</code> node and set the appropriate date and time values used during Unit Testing.</p> <p>For example, imagine two Transformer stage variables using the following expressions:</p> <pre><code>BeforeToday =\nIf inPurchases.Purchase_Date &lt; DSJobStartDate Then \"Yes\" else \"No\"\n\nEarlierToday = \nIf inPurchases.Purchase_Date = DSJobStartDate and \n   inPurchases.Purchase_Time &lt; DSJobStartTime Then \"Yes\" else \"No\"\n</code></pre> <p>The <code>when</code> section of your test specification would look like this:</p> <pre><code>{\n    \"given\": [\n        ...\n    ],\n    \"when\": {\n        \"data_intg_flow_ref\": \"blah-blah-blah\",  \n        \"parameters\": {\n            \"DSJobStartDate\": \"2012-01-15\",\n            \"DSJobStartTime\": \"11:05:01\"\n        }\n    },\n    \"then\": [\n        ...\n    ]\n}\n</code></pre> Be careful when using multiple date/time macros <p>Exercise caution when setting <code>DSJobStartTimestamp</code> in conjunction with either <code>DSJobStartDate</code> or <code>DSJobStartTime</code> as the DataStage test case capability does not attempt to enforce logical consistency between these parameters. i.e. You could specify conflicting values for these macros which would create a contradictory logical condition. e.g. <pre><code>{\n    \"when\": {\n        \"data_intg_flow_ref\": \"blah-blah-blah\",  \n        \"parameters\": {\n            \"DSJobStartTimestamp\": \"2025-02-03 00:00:00\", \n            \"DSJobStartDate\": \"2024-04-05\"\n            \"DSJobStartTime\": \"09:30:00\"\n        }\n    }\n}\n</code></pre></p>","tags":["DataStage","Creating Tests"]},{"location":"pipelines/example-pipepline/","title":"MettleCI Example Pipeline for DevOps","text":"<p>MettleCI ships with a set of example pipelines which demonstrate how to use the MettleCI command line to compose a pipeline which satifies a number of scenarios.</p> <p>This example uses a screenshot from Jenkins (below). From a MettleCI perspective, the process is identical regardless of the build tool being used.</p> <p>Jenkins, unlike other automation platforms has split the Build and Deployment sections of the process into their own separate pipelines.</p> <p>Note: Jenkins also forces a Java version incompatibility issue upon the MettleCI CLI, whereby the agent running the pipelines (and therefore the MettleCI CLI) does not support Java 8, which is required by the MettleCI CLI. We resolve this issue in our pipelines by instructing Jenkins to explicitly set the Java version when running scripts, but it results in additional messages at the beginning of every stage that we have removed from this documentation for the sake of clarity.</p>","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#devops-build","title":"DevOps Build","text":"<p>When an update has been committed and pushed to the MettleCI project repository, the Build pipeline is called automatically.</p>","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#1-diagnostics","title":"1 - Diagnostics","text":"<p>This stage is simply a diagnostic step to help with initial development of your pipeline.  Once you have confirmed that your pipeline is behaving as required you can safely delete this step.</p> # Task Command Description 1 Diags 11.7 various Uses the Build Agent to log environment variables and variable groups for diagnostics purposes.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#2-deploy-ci","title":"2 - Deploy CI","text":"<p>Deploy to Continuous Integration</p> <p>This Job is defined in pipeline template mci_deploy.groovy</p> # Task Command Description 1 Create DataStage Project <code>mettleci datastage create-project</code> (documentation) Verify that the CI project exists (otherwise, create it). 2 Substitute parameters in DataStage config <code>mettleci properties config</code> (documentation) Replace variables in the <code>DSParams</code> file, any Parameter Sets files, and any user-define shell scripts (from repository path/datastage) with values defined in the var.ci properties file.  Processed files are stored in a config directory created by this command in the current workspace on the agent. This example operates on <code>*.sh</code> files in the <code>datastage</code> base directory, the <code>DSParams</code> file, and all parameter sets using values from the file <code>var.ci</code> (as determined by the <code>environmentId</code> parameter.)  Substituted files are created in a new temporary build directory on your build agent host called <code>config</code> 3 Cleanup temporary files from previous builds <code>mettleci remote execute</code> (documentation) Execute your datastage/cleanup.sh script. This script removes the directory containing previous pipeline artefacts on the datastage engine, ensuring contains only artefacts created for this execution are present. 4 Transfer DataStage config and filesystem assets <code>mettleci remote upload</code> (documentation) Upload the config directory containing files modified by the Configure Properties step and your repository\u2019s filesystem directory from the build agent to your CI environment. 5 Deploy DataStage config and file system assets <code>mettleci remote execute</code> (documentation) Execute your config/deploy.sh script (previously generated by the Configure properties task from your repository's datastage/deploy.sh file) on your CI environment. This user-supplied script performs solution-specific deployment processes required by the assets in the filesystem directory. 6 Deploy DataStage project <code>mettleci datastage deploy</code> (documentation) Perform an incremental deployment of your DataStage assets to the CI project. 7 Cleanup <code>mettleci remote execute</code> (documentation) Execute the script config/cleanup.sh (previously generated by the Configure properties task from your repository's datastage/cleanup.sh file) in the CI environment. This script removes files which are no longer required from your build agent\u2019s disk. 8 Find files in the workspace Inline script Detects and records (in a build pipeline variable) whether the Job compilation performed by the Deploy DataStage assets task produced an output JUnit .xml file. Note that the incremental deployment approach used by MettleCI may mean that a test output file would not be generated in the event that the deployment process does not detect any changes. 9 Archive JUnit-formatted test results Build tool specific If a JUnit <code>.xml</code> file was detected by the Find files in the workspace step then publish it to the build tool for use in test suite management.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#3-test-ci","title":"3 - Test CI","text":"","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#flow-anlaysis-ci-test-warnings","title":"Flow anlaysis CI test warnings","text":"<p>This Job is defined in pipeline template <code>mci_compliance.groovy</code>. *** JENKINS ONLY, RIGHT?! ***</p> # Task Command Description 1 Check out from version control Build tool specific Make the Compliance repository (separate to the DataStage asset repository, where this pipeline definition is stored) accessible to the build agent. 2 Compliance Test - Warnings <code>mettleci compliance test -ignore-test-failures</code> (documentation) Invokes MettleCI Compliance checks for changes jobs.  Note that in this mode (including the -ignore-test-failures option) a failed Compliance check will not cause the entire build pipeline to fail. 3 Find files in the workspace Build tool specific Detects and records (in a build pipeline variable) whether the Compliance check performed by the Run Compliance task produced an output JUnit <code>.xml</code> file. Note that the incremental deployment approach used by MettleCI may mean that a test output file would not be generated in the event that the deployment process does not detect any changes. 4 Archive JUnit-formatted test results Build tool specific If a JUnit <code>.xml</code> file was detected by the Check for results step then publish it to the build tool for use in test suite management.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#flow-anlaysis-ci-test-failures","title":"Flow anlaysis CI test failures","text":"<p>This Job is defined in pipeline template <code>mci_compliance.groovy</code>  *** JENKINS ONLY, RIGHT?! ***</p> # Task Command Description 1 Check out from version control Build tool specific Make the Compliance repository (separate to the DataStage asset repository, where this pipeline definition is stored) accessible to the build agent. 2 Compliance Test - Errors <code>mettleci compliance test</code> (documentation) Invoke MettleCI Compliance checks for changes jobs.  Note that in this mode (omitting the -ignore-test-failures option) a failed Compliance check will cause the entire build pipeline to fail. 3 Find files in the workspace Inline script Detects and records (in a build pipeline variable) whether the Compliance check performed by the Run Compliance task produced an output JUnit <code>.xml</code> file. Note that the incremental deployment approach used by MettleCI may mean that a test output file would not be generated in the event that the deployment process does not detect any changes. 4 Archive JUnit-formatted test results Build tool specific If a JUnit <code>.xml</code> file was detected by the Check for results step then publish it to the build tool for use in test suite management.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#datastage-test-cases","title":"DataStage test cases","text":"<p>This Job is defined in pipeline template <code>mci_unittest.groovy</code>  *** JENKINS ONLY, RIGHT?! ***</p> # Task Command Description 1 Configure Properties <code>mettleci properties config</code> (documentation) Replace variables in the cleanup_unittest.sh file, with values defined in the var.ci properties file. Substituted files are created in a new temporary build directory on your build agent host called 'config'. 2 Cleanup <code>mettleci remote execute</code> (documentation) Execute the script stored in the repository file config/cleanup_unittest.sh (previously generated by the Configure properties task) in the CI environment.  This script removes unit test results, to ensure a clean environment for the next execution. 3 Upload unit test specs <code>mettleci remote upload</code> (documentation) Upload your repository\u2019s unittest directory from the build agent to your CI environment. 4 Create unit test report dir <code>mkdir unittest-reports</code> Create directory to receive unit test outputs. 5 Run Unit Tests <code>mettleci unittest test</code> (]documentation]()) Invoke the Unit Tests defined in your unittest directory.  Note that MettleCI will use its incremental detection functionality to only execute Unit Tests where either the Unit Test case or associated Job have changed. 6 Download unit test reports <code>mettleci remote download</code> (documentation) Download any JUnit .xml files produced by the Run Unit Tests task from your CI environment to the build agent. 7 Find files in the workspace Inline script Detects and records (in a build pipeline variable) whether the Unit Test execution performed by the Run Unit Tests task produced an output JUnit .xml file. Note that the incremental deployment approach used by MettleCI may mean that a Unit Test output file would not be generated in the event that the deployment process does not detect any changes. 8 Archive JUnit-formatted test results Build tool specific If a JUnit .xml file was detected by the Check for results step then publish it to the build tool for use in test suite management.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#4-tag-repository","title":"4 - Tag repository","text":"<p>If the build completed successfully, tag the repository.  *** WITH WHAT?! ***</p> # Task Command Description 1 Tag current build various Adds a tag to the commit that triggered the successful build, thereby identifying it as a candidate for deployment into the QA environment. The tag is in the format ci-{build_number}.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#devops-deploy","title":"DevOps Deploy","text":"<p>Once the DevOps Build pipeline has run successfully for a particular checkin, it will have been marked as a candidate for deployment into the first of the testing environments used by the MettleCI example pipelines: QA.</p> <p>The DevOps Deploy pipeline is triggered manually when one of the possible candidate tags is chosen to be deployed.</p> <p>The MettleCI example pipelines use different environments to illustrate a promotion path from Development (and automatic Compliance and Unit Testing in CI via the Build pipeline), then QA, Perf and finally Prod. Unlike the pipelines for other supported platforms, Jenkins separates Build from Deployment, but also only deploys to one environment per running of the DevOps Deploy pipeline, using the repository tags to determine what can be promoted..</p>","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#1-select-release-to-deploy","title":"1 - Select release to deploy","text":"<p>This first step determines which tag is being promoted to which environment.</p> <p>The first step is to choose the target environment, out of <code>qa</code>, <code>perf</code> and <code>prod</code>.</p> <p>Based on the chosen target environment, the user is presented with a list of candidate tags that enforce the proper progression towards Production:</p> <ul> <li><code>qa</code> - user is only presented with tags starting with 'ci-'</li> <li><code>perf</code> - only 'qa-' tags</li> <li><code>prod</code> - only 'perf-' tags</li> </ul> <p>*** THIS WORKS DIFFERENTLY FOR DIFFERENT BUILD SYSTEMS, RIGHT?! ***</p> <p>Given the fact that a deployment into the Perf environment has to be successful in order to be tagged with a \u201cperf-\u201d tag and only \u201cperf-\u201d tags can be deployed into Production, this ensures that code cannot be deployed directly into Production.</p> # Task Command Description 1 Tag current build various Calls upon the the user to choose the target environment, and then choose the desired tag from the list of approved candidates.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#2-deploy-target","title":"2 - Deploy target","text":"<p>This Job is defined in pipeline template mci_deploy.groovy</p> # Task Command Description 1 Create DataStage Project <code>mettleci datastage create-project</code> (documentation) Verify that the CI project exists (otherwise, create it). 2 Substitute parameters in DataStage config <code>mettleci properties config</code> (documentation) Replace variables in the DSParams file, any Parameter Sets files, and any user-define shell scripts (from repository path /datastage) with values defined in the var.ci properties file.  Processed files are stored in a config directory created by this command in the current workspace on the agent. This example operates on *.sh files in the 'datastage' base directory, the DSParams file, and all Parameter Sets using values from the file 'var.ci' (as determined by the environmentId parameter). Substituted files are created in a new temporary build directory on your build agent host called 'config' 3 Cleanup temporary files from previous builds <code>mettleci remote execute</code> (documentation) Execute your datastage/cleanup.sh script. This script removes the directory containing previous pipeline artefacts on the datastage engine, ensuring contains only artefacts created for this execution are present. 4 Transfer DataStage config and filesystem assets <code>mettleci remote upload</code> (documentation) Upload the config directory containing files modified by the Configure Properties step and your repository\u2019s filesystem directory from the build agent to your CI environment. 5 Deploy DataStage config and file system assets <code>mettleci remote execute</code> (documentation) Execute your config/deploy.sh script (previously generated by the Configure properties task from your repository's datastage/deploy.sh file) on your CI environment. This user-supplied script performs solution-specific deployment processes required by the assets in the filesystem directory. 6 Deploy DataStage project <code>mettleci datastage deploy</code> (documentation) Perform an incremental deployment of your DataStage assets to the CI project. 7 Cleanup <code>mettleci remote execute</code> (documentation) Execute the script config/cleanup.sh (previously generated by the Configure properties task from your repository's datastage/cleanup.sh file) in the CI environment. This script removes files which are no longer required from your build agent\u2019s disk. 8 Find files in the workspace Inline script Detects and records (in a build pipeline variable) whether the Job compilation performed by the Deploy DataStage assets task produced an output JUnit <code>.xml</code> file. Note that the incremental deployment approach used by MettleCI may mean that a test output file would not be generated in the event that the deployment process does not detect any changes. 9 Archive JUnit-formatted test results Build tool specific If a JUnit .xml file was detected by the Find files in the workspace step then publish it to the build tool for use in test suite management.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#3-tag-repository","title":"3 - Tag repository","text":"<p>If the build ran successfully, tag the repository.</p> # Task Command Description 1 Tag current build various Adds a tag to the commit that triggered the successful build, thereby identifying it as a candidate for deployment into the next  environment. i.e: if the current run successfully deployed a <code>ci-xxx</code> tag into <code>QA</code>, the commit the tag was related to also receives a <code>qa-xxx</code> tag, identifying it as a potential candidate for deployment into the <code>PERF</code> environment.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#4-create-hotfix-branch","title":"4 - Create hotfix branch","text":"<p>The Jenkins DevOps pipeline example is extended to cover a Production hotfix scenario, whereby hotfix changes can be made against a copy of the Production DataStage project. This allows for quick turnaround on critical Production issues without attempting to fix DataStage jobs directly in Production.</p> # Task Command Description 1 Building Jenkins-DevOps-Hotfix-Create various If the current deployment was into the Production environment, run a separate Jenkins pipeline that updates a special hotfix DataStage project to always be in line with the Production project, and also create a separate branch in the git repository to manage any hotfix changes that may be made.","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/example-pipepline/#notes","title":"Notes","text":"<p>Note that every build Job has implicit access to the DataStage asset repository where this pipeline definition is stored.  Some build tools make this explicit and produce a log entry showing this.  For example Azure DevOps automatically introduces a task at the beginning of each Job with a title for the form Checkout <code>&lt;repository-name@&lt;branch&gt;</code> to <code>s\\&lt;project-name&gt;</code>.</p> <p>Below is the Azure version of the complete DevOps pipeline. This illustrates the way the other automation platform pipelines are structured in terms of the Build and Deploy processes. Arranging them in a continuous pipeline with explicit approval  each deployment step (not including CI) enforces the order of deployment through to Production.</p>","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/filesystem-deployment/","title":"Filesystem deployment","text":"","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#introduction","title":"Introduction","text":"<p>MettleCI will handle the management and deployment of DataStage Assets, Project Environment Variables and Parameter Sets.  However, most ETL processes consistent of more than just DataStage jobs and will usually require additional file system assets in order to function correctly.  Just like DataStage jobs and sequences, these file system assets should be checked into version control and automatically deployed and tested as part of your MettleCI deployment pipeline.</p> <p>Examples of file system assets which typically form part of an ETL solution:</p> <ul> <li>Shell Scripts</li> <li>DataStage schema files</li> <li>SQL Scripts</li> <li>Directory Structures</li> <li>Reference Data Files</li> <li>Configuration Files</li> </ul> <p>Unlike DataStage assets, the process of deploying file system assets is highly dependent on the design of each ETL solution.  Unfortunately, the heterogeneous nature of file system assets makes it all but impossible for MettleCI to include a one-size-fits-all deployment process.  Instead, MettleCI provides a simple, extensible mechanism to quickly build your own file system asset deployment process which is executed by MettleCI.</p> <p>This guide will first explain how file system asset deployments are performed by MettleCI and then discuss some best practices intended to ensure your automated deployments are both repeatable and easy to maintain.</p>","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#deployment-process","title":"Deployment Process","text":"<p>Regardless of whether a deployment is being performed as part of Continuous Integration or Release Deployment, MettleCI will complete the following steps:</p> <ol> <li>Create target DataStage project (if it doesn\u2019t exist)</li> <li>Apply environment specific changes to DSParams and deploy to target DataStage project</li> <li>Apply environment specific changes to Parameter Set Value files and deploy to target DataStage project</li> <li>Execute file system deployment</li> <li>Import DataStage assets</li> <li>Compile DataStage assets</li> </ol> <p>File system deployments are executed after the DataStage project has been created but before the DataStage Import and Compile steps are triggered.  This means that at the time of file system deployment, the target DataStage project directory will already exist while still allowing the deployment of any file system assets which might need to be in place before DataStage compilation commences (eg. Header files used by Build Ops or Basic Routines).</p> <p>Everything required for file system deployment lives within the filesystem directory inside the Git repository for the DataStage project being deployed.  At a minimum, the filesystem components of a DataStage project Git directory structure should contain:</p> <pre><code>\ud83d\udcc1\n\u251c\u2500\u2500 datastage\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 DSParams\n\u2514\u2500\u2500 filesystem\n \u00a0\u00a0 \u2514\u2500\u2500 deploy.sh\n</code></pre> <p>During file system deployment, MettleCI will (recursively) transfer the entire contents of the filesystem directory to a temporary directory on the DataStage engine that hosts the target DataStage project.  MettleCI will then source dsenv and execute the deploy.sh script on the DataStage engine with the following arguments:</p> <pre><code>deploy.sh -p &lt;datastage project&gt; -e &lt;environment&gt;\n</code></pre> <p>The idea is that all file system assets are checked into the filesystem directory in Git and the deploy.sh script will be called during deployment to move the files into the correct location on the DataStage engine.  When deploy.sh is executed, the current working directory is set to be the previously-mentioned temporary directory. This allows any files checked into the filesystem directory to be referenced and accessed within deploy.sh through the use of relative paths.</p> <p>The parameters  and  refer to the name of the target DataStage project and the logical environment name (eg. CI, TEST, PROD, etc) respectively. Note that  should ideally align with the related var.* override file and the Environment postfix.","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#example","title":"Example","text":"<p>As a simple example of how filesystem deployments should work, consider a DataStage project that depends on the following directory structure in order to function:</p> <pre><code>\ud83d\udcc1\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 example_prod\n     \u00a0\u00a0 \u251c\u2500\u2500 scripts\n     \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 wait_for_file.sh\n     \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 transfer_file.sh\n     \u00a0\u00a0 \u251c\u2500\u2500reference\n     \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 valid_countries.csv\n     \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 system_accounts.csv\n     \u00a0\u00a0 \u251c\u2500\u2500 input\n     \u00a0\u00a0 \u251c\u2500\u2500 output\n        \u2514\u2500\u2500 transient\n</code></pre> <p>The following table describes the purpose of each of directory:</p> Directory Purpose scripts Contains all shell scripts required by this project. reference Contains static reference data used for lookups during job execution. input Folder where external system will place input files processed by this project. output Folder where data produced by this project is placed for consumption by external systems. transient Folder containing transient files created during execution of this project but not required after the current ETL batch has completed. <p>To automatically deploy this file system with MettleCI, we first need to check in all the scripts and reference files into Git somewhere under the filesystem directory. The *.sh and *.csv file could be checked directly into the root of the filesystem directory structure but maintenance is easier if the Git directory structure closely matches the deployed file system.  We also need to check-in deploy.sh, resulting in a directory structure in Git which looks like this:</p> <pre><code>\ud83d\udcc1\n\u251c\u2500\u2500 datastage\n\u2514\u2500\u2500 filesystem\n    \u251c\u2500\u2500 scripts\n    \u2502   \u251c\u2500\u2500 wait_for_file.sh\n    \u2502   \u2514\u2500\u2500 transfer_file.sh\n    \u251c\u2500\u2500 reference\n    \u2502   \u251c\u2500\u2500 valid_countries.csv\n    \u2502   \u2514\u2500\u2500 system_accounts.csv\n    \u2514\u2500\u2500 deploy.sh\n</code></pre> This example excludes the <code>input</code>, <code>output</code> and <code>transient</code> directories in Git <p>This is because 1. Git only version-controls files, not directories since, strictly speaking, a directory is a property of a file; and 1. The deploy.sh script creates these folders at deploy time, as the following instructions explain.</p> <p>Finally, the content of the <code>deploy.sh</code> shell script would be:</p> <pre><code>#!/bin/sh\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n# Validate input arguments\n# \nUSAGE=\"${0} syntax:\n-p &lt;project name&gt; -e &lt;environment&gt;\"\nwhile getopts e:w: OPT\ndo\n    case $OPT in\n    p)  PROJECT_NAME=${OPTARG} ;;\n    e)  ENV_NAME=${OPTARG} ;;\n    ?)  echo ${USAGE}\n        exit -1 ;; \n    esac\ndone\n# Environment required\nif [[ \"${ENV_NAME}x\" = \"x\" ]]; then\n    echo \"ERROR: Parameter &lt;environment&gt; is required.\\n${USAGE}\";\n    exit -1;\nfi\n# Project required\nif [[ \"${PROJECT_NAME}x\" = \"x\" ]]; then\n    echo \"ERROR: Parameter &lt;project name&gt; is required.\\n${USAGE}\";\n    exit -1;\nfi\n#------------------------------------------------------------------------------\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n# Deploy filesystem\n#\n# directory structures\nmkdir -p /data/${PROJECT_NAME}/scripts\nmkdir -p /data/${PROJECT_NAME}/reference\nmkdir -p /data/${PROJECT_NAME}/input\nmkdir -p /data/${PROJECT_NAME}/output\nmkdir -p /data/${PROJECT_NAME}/transient\n# scripts\nrm -rf /data/${PROJECT_NAME}/scripts/*\ncp -r ./scripts/* /data/${PROJECT_NAME}/scripts/\n# reference data\nrm -rf /data/${PROJECT_NAME}/reference/*\ncp -r ./reference/* /data/${PROJECT_NAME}/reference/\n# clear transient files when running Continuous Integration\nif [[ \"$ENV_NAME\" = \"CI\" ]]; then\n    rm -rf /data/${PROJECT_NAME}/transient\nfi\n</code></pre> <p>There are a few important things to note about this script:</p> <ul> <li>The first half of <code>deploy.sh</code> contains boilerplate validation code.  This can be re-used across all your <code>deploy.sh</code> scripts.</li> <li>All required directories are created if they don\u2019t already exist using <code>mkdir -p</code></li> <li>Rather than copy each Project script or reference file individually, deploy.sh deletes the existing content of the scripts and reference directories and replaces it with the files being deployed.  This keeps deploy.sh easy to maintain and understand and it also means that the file system is automatically cleaned up when files are removed from Git.</li> <li>The <code>${ENV_NAME}</code> variable can be used to perform deployment steps specific to a given environment.  In this example we clear the transient directory for Continuous Integration (CI), an explanation of why you may want to do this is covered in the Best Practices section.</li> </ul>","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#best-practices","title":"Best Practices","text":"","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#keep-git-file-system-directory-structure-as-close-to-the-deployed-directory-structure-as-possible","title":"Keep Git file system directory structure as close to the deployed directory structure as possible","text":"<p>When the file system directory in Git does not match that of the DataStage engine, developers will need to inspect the deploy.sh script to figure out how one directory structure maps to the other.   Keeping the Git and DataStage engine directory structure closely aligned keeps things simple and reduces debugging and maintenance effort.</p>","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#clear-and-deploy-directories-not-individual-files","title":"Clear and deploy directories, not individual files","text":"<p>Rather than writing deploy.sh to move each each and every file individually, deploy entire directories in one go by first performing an rm -rf and followed by cp -r (see scripts and reference data in the previous example).  This approach not only reduces how often the deploy.sh script needs to be changed but it also ensures that old files are automatically removed as part of the deployment process.  </p> <p>For example, imagine your DataStage project contained a script called legacy_script.sh that was originally checked into the Git directory /filesystem/scripts and which was removed in a later revision of the project.  If the deploy.sh script just had a long list of shell scripts to copy (including our legacy_script.sh), the legacy_script.sh file would never be removed when we deploy new versions.  Worse still, any ETL job or sequence still referring to that script would still pass in testing as legacy_script.sh may still exist on the file system.  By clearing the scripts directory on the DataStage engine, then copying all scripts from /filesystem/scripts in Git, legacy_script.sh would be removed automatically as part of deployment and any ETL jobs or sequences still referring to it would (correctly) fail during testing.</p>","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#dont-put-files-from-multiple-projects-in-one-directory-structure","title":"Don\u2019t put files from multiple projects in one directory structure","text":"<p>When a single directory structure contains files from multiple projects, it is impossible to clear files in a directory during one project deployment without negatively affecting the other projects.  Always ensure that a file can be identified as being \u201cowned\u201d by a particular project based on the directory structure or file naming convention.</p> <p>For example, lets say we have two DataStage projects with the following filesystem structure in Git:</p> <pre><code>\ud83d\udcc1\n\u251c\u2500\u2500 project_1\n\u2502   \u251c\u2500\u2500 datastage\n\u2502   \u2514\u2500\u2500 filesystem\n\u2502       \u251c\u2500\u2500 scripts\n\u2502       \u2502   \u251c\u2500\u2500 copy_files.sh\n\u2502       \u2502   \u2514\u2500\u2500 rename_file.sh\n\u2502       \u2514\u2500\u2500 deploy.sh\n\u2514\u2500\u2500 project_2\n    \u251c\u2500\u2500 datastage\n    \u2514\u2500\u2500 filesystem\n        \u251c\u2500\u2500 scripts\n        \u2502   \u251c\u2500\u2500 wait_for_file.sh\n        \u2502   \u2514\u2500\u2500 transfer_file.sh\n        \u2514\u2500\u2500 deploy.sh\n</code></pre> <p>If both of these projects deployed their scripts directory to a common directory on the DataStage engine called <code>/data/scripts/</code>, it would contain <code>copy_files.sh</code>, <code>rename_files.sh</code>, <code>wait_for_file.sh</code> and <code>transfer_file.sh</code>.  Therefore it would not be possible for the <code>project_1</code> deployment process to clear all the scripts without also removing scripts from <code>project_2</code>.  Additionally, if both projects happened to have a script of the same name, the version in the scripts directory will change depending on which project was most recently deployed.</p> <p>By ensuring the DataStage engine directory structure is separated by project (e.g., <code>/data/scripts/&lt;project&gt;/</code>, <code>/&lt;project&gt;/scripts/</code>, <code>/data/&lt;project&gt;/scripts/</code>, etc.), it's clear which project 'owns' the scripts.  Otherwise, an approach which is less conventional but just as effective is to copy scripts to a common directory but to add a project identifier prefix or suffix to each file's name.  In this case, the <code>/data/scripts/</code> directory could contain <code>project1_copy_file.sh</code>, <code>project1_rename_files.sh</code>, <code>project2_wait_for_file.sh</code> and <code>project2_transfer_file.sh</code>.  The <code>deploy.sh</code> script could then clear the scripts directory by running <code>rm -rf /data/scripts/&lt;project&gt;_*</code>.</p>","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#clear-transient-files-that-are-created-and-used-in-a-single-etl-batch","title":"Clear transient files that are created and used in a single ETL batch","text":"<p>Most ETL processes are composed of multiple DataStage jobs which are executed in sequence and communicate by writing to and reading from files.  When these files are transient and are only expected to live for the life of a single batch, it is good practice to remove them as part of the file system deployment process.   Doing so ensures that problems introduced when renaming files or removing upstream jobs are quickly identified during test execution.</p> <p>For example, consider the following simple ETL process consisting of three jobs and the resulting design when <code>Job B</code> is removed</p> <p></p> <p>If transient files are not removed as part of the deployment process, then <code>Job C</code> will continue to run (invalidly) during testing because <code>File B</code> would still exist on the file system from previous executions.  By deleting all transient files (<code>File A</code> and <code>File B</code>), running the ETL process in testing would (validly) fail when executing Job C and the problem could be quickly detected and corrected.  The same issue can occur if a developer renames files written or read by jobs without ensuring all other jobs are correctly updated to reflect the change.</p>","tags":["Pipelines"]},{"location":"pipelines/filesystem-deployment/#be-careful-when-deleting-dataset-files","title":"Be careful when deleting Dataset files","text":"<p>As documented by IBM, the structure of a DataStage Dataset means that they cannot simply be deleted by removing just the descriptor file (often named with a *.ds extension).  One solution to clearing file system directories that may contain datasets is to use \u2026</p> <pre><code>find -name \u201c*.ds\u201d | xargs -l orchadmin delete\n</code></pre> <p>\u2026 to clean up any Datasets before performing <code>rm -rf</code> on a directory.  This is effective but can be quite slow.</p> <p>If all datasets are transient, an alternative approach is to ensure each project writes Dataset segments to its own directory using resource disks in the <code>$APT_CONFIG_FILE</code>.  Using this approach you can remove all <code>*.ds</code> files for the project using normal rm commands and the Dataset segment files can be removed by clearing the project specific resource disks.</p> <p>For example, consider the <code>$APT_CONFIG_FILE</code> for a DataStage project called example_prod:</p> <pre><code>{\n        node \"node1\"\n        {\n                fastname \"ds-engn.datamigrators.io\"\n                pools \"\"\n                resource disk \"/data/datasets/example_prod\" {pools \"\"}\n                resource scratchdisk \"/data/scratch\" {pools \"\"}\n        }\n        node \"node2\"\n        {\n                fastname \"ds-engn.datamigrators.io\"\n                pools \"\"\n                resource disk \"/data/datasets/example_prod\" {pools \"\"}\n                resource scratchdisk \"/data/scratch\" {pools \"\"}\n        }\n}\n</code></pre> <p>Because the configured resource disk contains only Dataset segment files used by our <code>example_prod</code> project and we know DataStage datasets are transient and are not required after ETL Batch execution is complete, we can simply perform <code>rm /data/datasets/example_prod</code> to clear all DataStage segment files for this project.  The remaining <code>*.ds</code> descriptor files can then be treated like regular files. </p>","tags":["Pipelines"]},{"location":"pipelines/pipelines/","title":"Build and deployment pipelines","text":"","tags":["Pipelines","CLI","Deploy","Build"]},{"location":"pipelines/repeatable-depoyments/","title":"Repeatable DataStage Project Deployments","text":"","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#introduction","title":"Introduction","text":"<p>CI/CD Pipelines will deploy a version of software across multiple environments while subjecting it to harder and harder tests in order to be confident that the software works as intended.  As a result, a core component of any CI/CD Pipeline is to be able to repeatably deploy a given version of software to multiple environments without introducing any differences.  Differences between software deployed across different testing environments can trigger random test failures, or worse, software that worked in testing can fail once deployed to production.</p> <p>MettleCI provides a set of composable deployment tools for performing repeatable and efficient DataStage Project deployments.</p>","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#deployment-of-job-designs","title":"Deployment of job designs","text":"","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#incremental-deployment","title":"Incremental Deployment","text":"<p>A simple but na\u00efve approach to deploying ISX files containing assets with design information would be as follows:</p> <ol> <li>Tear down any existing DataStage project in the target environment</li> <li>Create a new DataStage project in the target environment</li> <li>Import all ISX files</li> <li>Compile all jobs in the DataStage project.</li> </ol> <p>This approach is repeatable as the content of the resulting DataStage project will always match the software version represented by the source ISX files.  However, performance of this approach is extremely poor and will not allow the resulting CI/CD Pipelines to provide fast feedback to developers.  As an example, a DataStage project containing about 500 jobs will typically take over 1.5hrs to perform both the import and compile steps of this deployment approach.</p> <p>The implementation used by the MettleCI deploy command is logically equivalent to the na\u00efve approach but uses an incremental algorithm to deploy changes in a fast and scalable manner:</p> <ol> <li>Source ISX files and the existing DataStage project are analyzed for differences</li> <li>Differences are converted to operations which transform the existing DataStage project to match the source ISX files</li> <li>An impact analysis of changes caused by step 2 is performed and all affected jobs are compiled (eg. if a shared container changed, then all jobs using it will be compiled) </li> </ol> <p></p> <p>Duration of a deployment depends on the number of changes rather than the size of project.  For a typical CI/CD Pipeline that is triggered per commit, DataStage project deployments take a few minutes.</p>","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#example-scenario","title":"Example Scenario","text":"<p>Consider this example CI/CD Pipeline</p> <p></p> <p>Using deployments with designs, this pipeline works as follows:</p> <ul> <li>Deploys to DataStage Projects that represent CI, SIT, QA and Prod environments</li> <li>Continuous Integration performed in the CI environment is <ul> <li>Deployed from version control using Incremental Deployment</li> <li>Runs automated Static Analysis and Unit Tests</li> <li>Produces an release Artifact when successful</li> </ul> </li> <li>Manual Testing/Verification is performed in SIT, QA and Prod environments<ul> <li>Deployed from release Artifact using Incremental Deployment</li> <li>Manual tests could be replaced with automated tests</li> </ul> </li> </ul> <p>MettleCI\u2019s datastage deploy command would be used to perform the DataStage project deployments to the CI, SIT, QA and Prod environments.  In this scenario, the release Artifact will contain exactly the same ISX files as those checked out from Version Control for the purpose of Continuous Integration.  This is the recommended approach when building CI/CD Pipelines using MettleCI.</p>","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#deployment-of-job-binaries","title":"Deployment of job binaries","text":"<p>Some customers have additional constraints or requirements which prevent the use of the job designs for deployments other than Continuous Integration. Typical examples are:</p> <p>Compilers cannot be used in Production environments for security reasons</p> <p>Only DataStage job binaries can be deployed to Testing and/or Production environments</p> <p>While it is not the recommended approach for building CI/CD Pipelines using MettleCI, the following tools can be composed to perform DataStage project deployments with binaries.</p>","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#incremental-export-with-binaries","title":"Incremental Export with binaries","text":"<p>MettleCI is designed so that ISX files checked into Version Control contain designs without binaries.  In order to get ISX files which include binaries, the source ISX files need to be first imported into a DataStage project, compiled and then exported to ISX with binaries.</p> <p>A na\u00efve approach would be implemented as follows:</p> <ol> <li>Run a MettleCI Incremental Deployment using ISX files with designs</li> <li>Export the entire DataStage project with binaries</li> </ol> <p>The resulting export would have job designs which match the source ISX files while containing the required binaries.  Unfortunately, exporting an entire project is too slow and doesn\u2019t scale well as the DataStage project increases in size.  A project with about 500 jobs will typically take about 45 minutes to export with binaries.</p> <p>Instead, the DataStage project export can be performed by MettleCI\u2019s ISX export dommand which can optionally perform an incremental export with or without binaries.  Incremental Export reads from a DataStage project and exports its content into a  directory of ISX files.  If the export directory is empty then Incremental Export will export the entire DataStage project, otherwise only new or changed assets will be exported:</p> <ol> <li>Source DataStage project and target export directory containing ISX files are analyzed for differences</li> <li>Differences are converted to operations which update the ISX files in the target export directory </li> <li>When including binaries during export, an impact analysis of changes identified during step 2 is performed and all affected jobs are exported to capture the latest binaries (eg. a subflow component changed, so we export the binaries of all jobs which use it)</li> </ol> <p></p> <p>Duration of export depends on the number of changes rather than the size of project.  For a typical CI/CD Pipeline that is triggered per commit, DataStage project exports take a few minutes.</p>","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#incremental-import-with-binaries","title":"Incremental Import with binaries","text":"<p>An incremental import using the ISX Import Command is designed to accept a directory of ISX files and ensure that content of the target DataStage project will be updated to match the ISX files.  The process is very similar to an Incremental Deployment, the fundamental difference is that it doesn\u2019t automatically compile jobs and is able to deploy binaries from the ISX files:</p> <ol> <li>Source ISX files and the existing DataStage project are analyzed for differences</li> <li>Differences are converted to operations which transform the existing DataStage project to match the source ISX files</li> </ol> <p>Incremental Imports can also be configured to exclude design information.  When enabled, jobs will not be viewable/editable in the DataStage Designer/Flow Designer and any assets which are not required for execution (such as table definitions, shared containers, etc) will be excluded from the import.</p>","tags":["Pipelines"]},{"location":"pipelines/repeatable-depoyments/#example-scenario_1","title":"Example Scenario","text":"<p>Consider this example example CI/CD Pipeline</p> <p></p> <p>Using deployments with binaries, this pipeline works as follows:</p> <ul> <li>Deploys to DataStage Projects that represent CI, SIT, QA and Prod environments</li> <li>Continuous Integration performed in the CI environment is <ul> <li>Deployed from version control</li> <li>Runs automated Static Analysis and Unit Tests</li> <li>Performs Incremental Export with binaries</li> <li>Produces a release Artifact from exported binaries when Continuous Integration is successful</li> </ul> </li> <li>Manual Testing/Verification is performed in SIT, QA and Prod environments<ul> <li>Deployed from release Artifact using Incremental Import</li> <li>Manual tests could be replaced with automated tests</li> </ul> </li> </ul> <p>MettleCI\u2019s datastage deploy command is used to deploy the design only ISX files from Version Control.  When Continuous Integration completes successfully, ISX files with binaries are exported using the ISX Export Command  and an artifact is created from them.  All downstream deployments use the isx import command to perform an incremental import of binaries, this process does not require a compiler.  Optionally, incremental import can be configured to ignore job designs while deploying to any environment.  This means that the SIT and QA environments could be deployed with designs and binaries while production is deployed with just binaries.</p>","tags":["Pipelines"]},{"location":"pipelines/wip-hotfixes/","title":"Hotfixes","text":"","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/wip-selective-promotion/","title":"Page Template: Update this with your Page Title","text":"","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/wip-selective-promotion/#update-with-your-section-title","title":"Update with your section title","text":"<p>bold text Italic text</p> <p>List:</p> <ul> <li>Item 1</li> <li>Item 2</li> <li>Item 3</li> </ul> <p>Ordered List:</p> <ol> <li>Item 1</li> <li>Item 2</li> <li>Item 3</li> </ol> Collapsible Section <p>Section content</p> <p>Link to IBM Home Page</p> <p>This is a sample code block <pre><code>Code Block here\n</code></pre></p>","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/wip-selective-promotion/#table","title":"Table","text":"Column 1 Column 2 Row 1 Row 1 Row 2 Row 2","tags":["TAG1","TAG2","TAG3"]},{"location":"pipelines/wip-selective-promotion/#references","title":"References","text":"<ul> <li>Source 1</li> <li>Source 2</li> </ul>","tags":["TAG1","TAG2","TAG3"]},{"location":"railroads/","title":"Python grammar railroad diagrams","text":"<p>Requirements:  - <code>cairo</code>, e.g. <code>brew install cairo</code></p> <p>Run: <pre><code>$ python main.py\n</code></pre></p>"},{"location":"railroads/#credit","title":"Credit","text":"<p>railroad.py is adapted from tabatkins/railroad-diagrams.git</p>"},{"location":"testing/baselining-test-results/","title":"Recapture a test result baseline","text":"<p>As the logic of your flow changes, so the test data files representing your desired output will also need to change.  DataStage\u00ae can automatically re-baseline your expected output based on an execution of your DataStage job.  Note that while this process is similar to the process described in Capturing test data this process differs in that it only captures the output(s) of your job.</p> <p></p>","tags":["DataStage","Creating Tests"]},{"location":"testing/baselining-test-results/#process","title":"Process","text":"<ol> <li>In the test case editor, open the Test data in use tree and select the test output file(s) you want to re-baseline.</li> <li>In the data area above the table click the trash icon to delete the data file.</li> <li>Your test specification now refers to at least one CSV data file which no longer exists. Execute your test case without changing the specification. DataStage will identify that no expected results exist for the file(s) you\u2019ve deleted and re-create them using the data captured at runtime.  Note that test case will fail when executed in this mode.</li> <li>Re-execute the same test case job.  DataStage will now use the new baseline results files and your test case will pass.</li> </ol>","tags":["DataStage","Creating Tests"]},{"location":"testing/capturing-test-data/","title":"Capturing test data","text":"<p>Each DataStage\u00ae development team will already have a set of test data that they use to verify their flow's correct operation.  DataStage\u00ae enables you to capture that data (regardless of whatever technology is currently used to supply it) and encapsulate it into a commonly managed, well governed artefact which can travel with your flow to repeatedly assert its consistent behavior in any downstream environment.  This existing test data can be captured by DataStage by running a flow in 'Data Capture' mode.</p> <p></p> <p>This process involves running your flow in a 'Capture' mode.  In this mode DataStage will interrogate the data flowing along the input and output links referenced in your test case specification and record the data it observes into the test data file defined (by your specification) for each link.  This permits the capture of structured and unstructured data from both batch and streaming data sources.</p> <p>The data flowing along a flow's output links is captured as the current definition of 'expected' output into the relevant output data files.  When you alter the flow's functionality you may well need to re-capture a new baseline of expected results.</p>","tags":["DataStage","Creating Tests","Capture"]},{"location":"testing/capturing-test-data/#process","title":"Process","text":"<ol> <li>In the test case editor click Capture data. You'll need to accept any 'Overwrite all test data' warnings you receive by clicking the Capture data button.</li> <li>You'll receive a message telling you the flow is running.  Select the Test history tab to browse the most recent history of test case job invocations, including jobs currently in progress.</li> <li>Once the job is complete select the Data tab and browse the newly-populated test data under the the Test data in use tree.</li> </ol>","tags":["DataStage","Creating Tests","Capture"]},{"location":"testing/configuring-test-data-storage/","title":"Configuring test data storage","text":"","tags":["DATASTAGE","TESTING"]},{"location":"testing/configuring-test-data-storage/#create-a-connection","title":"Create a connection","text":"<p>Start by creating a connection to a storage resource where test assets will be stored.</p> <ol> <li>From the CPD Cluster home page open an existing project or create a new project then, on the Assets tab, click New Asset &gt; Connect to a data source.</li> <li>On the resulting page select one of the storage connection types supported by test cases and Next. Connection typesd currently supported as a DataStage test data connector are:<ul> <li>Storage volume,</li> <li>Google Cloud Storage, </li> <li>IBM Cloud Object Storage,</li> <li>Microsoft Azure Blob Storage, and </li> <li>Generic S3, which should be used instead of the Amazon S3 connector which is not supported.</li> </ul> </li> </ol> <p>Using a Storage volume connection:</p> <ol> <li>On the Create connection: Storage volume page give your new connection a name and optional description.</li> <li>Select an available storage volume. If you don't already have an storage volume available \u2026</li> <li>click New volume.</li> <li>Select a namespace (the default will be fine).</li> <li>Give your storage volume a name and optional description.</li> <li>Select your preferred storage type and click Add.</li> <li>Under Personal credentials &gt; Input method select Enter credentials manually.</li> <li>Check the Use my platform login credentials then click Test connection.</li> <li>Assuming you have a successful connection, click Save.</li> </ol> <p>When using one of the cloud object storage connectors (i.e. not Storage Volume) then see the relevant documentation for the connection type you selected.  Remember to verify your connection, by clicking Test connection, before clicking Save.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/configuring-test-data-storage/#add-the-connection-to-your-project-settings","title":"Add the connection to your project settings","text":"<p>Next you'll configure DataStage\u00ae to use your test data connection for storing test case assets.</p> <ol> <li>From the CPD Cluster home page select the Manage tab then, under Tools, select DataStage and then select the tab Test cases.</li> <li>For Test data connection type select the type of test data storage connector you created in the step above.</li> <li>For Test data storage select the name of your test data connection.</li> <li>For Default DataStage Test case job suffix specify a suffix which will be appended to all test case jobs to help distinguish them in the job log from invocations of their associated flows. e.g. <code>DataStage Test Case Job</code>. Note that you may wish to include a leading space as DataStage will not automatically add one for you.</li> </ol>","tags":["DATASTAGE","TESTING"]},{"location":"testing/configuring-test-data-storage/#sharing-a-test-data-connection-across-projects","title":"Sharing a test data connection across projects","text":"<p>Rather than creating individual test data connections for each of your projects you may prefer to share your test data connection across projects by adding it to the catalog.  This allows you to share the connection with other projects in your organization, making it easier to manage and maintain your test data connections.  Note that sharing test data connections in this way will not permit multiple projects to access each other's test data, despite them sharing an underlying storage mechanism. </p> <p>To share a test data connection using the catalog:</p> <ol> <li>From your project asset browser select your test data connection asset and use the vertical overflow menu () to select Publish to catalog.</li> <li>On the resulting dialog select the Platform assets catalog and click Next.</li> <li>On the next step of the wizard set any other preferences you wish and click Next.</li> <li>Review your selections and click Publish. </li> <li>From the hamburger menu in the top left of the CPD interface, select All catalogs and then select the Platform assets catalog.</li> <li>Use the vertical overflow menu () alongside the connection you want to share and click Add to project.</li> <li>Select the project with which you want to share the connection and click Next then Add.  Repeat this step for each project you want to share the connection with.  Note that you can only add a connection to a project if you have the Project Admin role in that project. </li> </ol> <p>Once you have configured a connection to a storage volume, you can use it to create test cases.  For more information see Creating DataStage test cases.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/creating-datastage-test-cases/","title":"Creating a DataStage test case","text":"<p>Unit tests can be created for DataStage\u00ae flows using the DataStage user interface.  When DataStage creates a test case it performs the following steps:</p> <ol> <li>Inspect your flow design to identify all source and target stages.</li> <li>Read the metadata definition of each source stage input link and target stage output link (i.e. all data flowing into and out of your flow).  Note that each source stage may be configured with multiple output links, and each target stage may be configured with multiple input links.</li> <li>Create an empty test case data file for each source and target link, with appropriate metadata definitions.</li> <li>Interrogate your flow's parameters.</li> <li>Create a test case specification which provides references to all of your flow's parameters as well as each newly-created test data file.</li> </ol> <p></p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/creating-datastage-test-cases/#process","title":"Process","text":"<p>You can create DataStage test cases either as a new asset or directly from within the DataStage designer canvas.</p> <p>From the CPD Cluster home page open an existing project or create a new project then, on the Assets tab, click New Asset &gt; Create reusable DataStage components &gt; Test case.</p> <p>To edit a test case from the DataStage canvas, go to an existing or new flow and click the Test cases icon to open up the test cases panel. Click New test case.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/creating-datastage-test-cases/#defining-test-case-properties","title":"Defining test case properties","text":"<ol> <li>On the Create test case page specify the name and optional description for the DataStage test case.  If you invoked this action when creating a new asset you'll also need to specify a DataStage flow with which to associate this test case.</li> <li>Click Next.</li> <li>On the Select stubbed links page select the flow links which will be stubbed in the test. This determines the \u2026<ul> <li>input links into which test data will be injected, and</li> <li>output links from which data will be compared to the expected output.</li> </ul> </li> <li>Click Next</li> <li>Specify the names of parameters or parameter sets for which your test will supply hard-coded values.</li> <li>Click Create.</li> </ol>","tags":["DATASTAGE","TESTING"]},{"location":"testing/creating-datastage-test-cases/#creating-test-data","title":"Creating test data","text":"<p>There are a number of ways you can derive data for your test case.  Start by opening you test case and from the Data tab selecting the test data file you wish to edit.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/creating-datastage-test-cases/#capture-test-data","title":"Capture test data","text":"<p>Use this method to capture data at runtime from a job invocation.  For more details see Capturing test data.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/creating-datastage-test-cases/#import-test-data","title":"Import test data","text":"<p>You can import locally-stored CSV data into your IBM Cloud Pak DataStage test data files:</p> <ol> <li>Click the Import button above the test data table.</li> <li>Upload your file by dragging and dropping your file, or clicking the link to specify a file.</li> <li>Click Import.</li> </ol> <p>Note that any existing test data in your selected file will be overwritten.</p> <p>You can read more about supplying the data and parameters that define your tests in editing DataStage test cases.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/","title":"Editing a DataStage test case","text":"<p>You can enter the DataStage\u00ae test cases editor either by selecting the Test case asset on the project page or from within the DataStage designer canvas.</p> <ul> <li>Open an existing project then open the Assets tab and under the Asset types panel select DataStage components &gt; Test cases from where you can click the name of the test case you wish to edit.</li> <li>From the DataStage canvas open an existing DataStage flow then click the Test cases icon to open up the test cases side panel from where you can click the name of the test case you wish to edit.</li> </ul> <p>This will bring you to the test case editor which allows you to inspect and modify your test case specification as well as edit the contents of your input and output test data files.</p> <p></p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#editing-the-test-case-specification","title":"Editing the test case specification","text":"<p>Selecting Specification from the Data tab will display the test case's specification.</p> <p>For a detailed explanation of the format of this JSON specification and the options available for controlling how your test case behaves see Test specification format.</p> <p>Each test data file referenced in your JSON specification (whether it represents input data or expected output data) will appear as a file under the Test data in use tree.  Selecting one of these will enable you to view and edit the test data.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#editing-test-data","title":"Editing test data","text":"<p>As well as using capture and import methods to derive test data, you can also manually enter test data into your test data files.  The test data table can be edited interactively like a spreadsheet and provides the following notable capabilities:</p> <ul> <li>Undo/redo of editing actions is supported with the traditional keyboard shortcuts Control-Z and Shift-Control-Z (Windows) or Command-Z and Shift-Command-Z (macOS).</li> <li>Values entered into a cell are validated based on the column's metadata definition. Invalid data is highlighted in red.</li> </ul>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#modifying-metadata","title":"Modifying metadata","text":"<ul> <li>Rows can be added by selecting the Add row icon above the test data table.</li> <li>Rows can be deleted by selecting the three dots on the row header and clicking Delete.</li> <li>Columns can be added by selecting the Add column icon above the test data table.</li> <li>Columns can be deleted by selecting the three dots on the column header and clicking Delete column.</li> </ul> <p>Modify the metadata of a test data column by selecting the three dots on the column header and clicking Edit column.  The resulting panel enables you to specify the column's name, data type, nullability, length, and extended metadata. Click Save to record your changes.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#modifying-test-data-outside-the-datastage-interface","title":"Modifying test data outside the DataStage interface","text":"<p>You can export IBM Cloud Pak DataStage test data files to locally-stored CSV files for offline editing:</p> <ol> <li>Click the menu overflow (V) button above the test data table to reveal more options and click Download .</li> <li>Modify the local filename as required and click Download.  The resulting CSV file will be downloaded to your browser's default download location.</li> </ol> <p>Once edited you can re-import your updated by clicking the Import button above the test data table and specifying your edited file before clicking Import.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#managing-test-cases","title":"Managing test cases","text":"","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#rename-a-test-case","title":"Rename a test case","text":"<p>You can rename a test case from the asset browser by moving your pointer over the test case name and clicking the edit icon which appears alongside it. Editing the name in the table cell and press the Return key or click the green checkmark to save your changes.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#delete-a-test-case","title":"Delete a test case","text":"<p>You can delete a test case from the asset browser by clicking the 'three dots' icon of the test case name and selecting Delete.  On the resulting dialog verify the relationships of the asset you're deleting and click Delete to confirm.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#save-changes","title":"Save changes","text":"<p>Once you're happy with your test data changes you can store your changes by clocking the Save button on the test data editor toolbar.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#export-a-test-case","title":"Export a test case","text":"<p>Export a test case to a zip file by clicking the Export icon on the test case toolbar.  On the resulting dialog confirm the name of the export file and click Download.  Note that this export does not contain test data.  To export test data for external modification see Modifying test data outside the DataStage interface above.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/editing-datastage-tests/#test-case-settings","title":"Test case settings","text":"<p>Click the Settings icon to open the test case settings panel which enables you to specify options for the selected test.</p> <p>History record storage allows you to specify the number of historical test results you wish to retain. This can be specified either by the number of days or the number of runs.</p> <p>Schedule allows you to specify a time and date when you want your test case job to be automatically executed and (optionally) the frequency with which the job should be re-executed.</p> <p>Click Save to store your selections.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/excluding-columns-from-tests/","title":"Excluding columns from tests","text":"<p>You can omit selected columns from the test case output comparison by adding the columns to be ignored to an <code>ignore</code> array in the relevant specification's <code>then</code> property.</p> <pre><code>{\n    \"then\": [\n        {\n            \"path\": \"ODBC_orders.csv\",\n            \"stage\": \"ODBC_order\",\n            \"link\": \"order_out\",\n            \"ignore\": [\n                \"Creation_date\",\n                \"Last_updated\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Note: Ignoring columns will prevent columns containing non-deterministic from affecting test results but will also omit those columns from test comparisons, so unexpected output in those columns, or changes in the output of those columns, will not be detected by your test case.  This reduces your test coverage and should be avoided if possible. A common technique to avoid non-deterministic outputs is to ensure all inputs data sources and flow parameters are stubbed by your test specification.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/executing-datastage-test-cases/","title":"Executing a DataStage test case","text":"<p>You can execute DataStage\u00ae test cases either from the CPD Cluster home page, the DataStage designer canvas, or the DataStage test case editor.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/executing-datastage-test-cases/#from-the-cpd-cluster-home-page","title":"From the CPD Cluster home page","text":"<ol> <li>Open an existing project and select the Jobs tab.</li> <li>Click the name of the test case you wish to invoke.  This will display the job details panel.</li> <li>Click the Run job button in the job details toolbar to invoke the test.</li> <li>When the status icon shows your test has completed click the timestamp to see test results.</li> </ol>","tags":["DATASTAGE","TESTING"]},{"location":"testing/executing-datastage-test-cases/#from-the-datastage-canvas","title":"From the DataStage canvas","text":"<ol> <li>Open an existing DataStage flow for which you have created a test case.</li> <li>Click the Test cases icon to  open up the test cases side panel.</li> <li>Click the Run icon alongside the name of the test case you wish to invoke.</li> <li>When the status icon shows your test has completed click the timestamp to see test results.</li> </ol>","tags":["DATASTAGE","TESTING"]},{"location":"testing/executing-datastage-test-cases/#from-the-datastage-test-case-editor","title":"From the DataStage test case editor","text":"<ol> <li>Open an existing test case.</li> <li>Click the Run button on the toolbar to invoke your test.</li> <li>Click View result on the notification that appears when your job is complete.</li> </ol> <p>Once your test is complete you should verify your test results.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/executing-datastage-test-cases/#background","title":"Background","text":"<p>When DataStage executes your test case it will dynamically replace your flow's input stages at runtime with components which inject data from the relevant CSV data files into your job on the links specified in your test specification.  Any source data repositories (files, databases, etc.) included in your test specification will not be connected to or read during a test case execution.</p> <p>Similarly, your flow's output stage(s) will be replaced at runtime with components which read the incoming data and compare it to the relevant CSV data files containing the output expected on those links.  Any differences are reported in the test results.</p> <p></p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/high-volume-tests/","title":"High Volume DataStage Tests","text":"<p>During the execution of a DataStage\u00ae test case the data produced by a job (on one or more output links) is compared against expected test data to identify and report on any differences.  When testing with large volumes of data, the comparison process may consume too much memory and cause your test job to abort with a fatal error.  The simplest approach to resolving this issue is to reduce your test data volume to the smallest number of records necessary to exercise each code path through your flow.  Doing so will ensure that your test cases execute quickly and can be easily understood and maintained.</p> <p>In the event that test data available to you cannot easily be reduced, the memory required by the data comparison process can be reduced by specifying a Cluster Key in the test specification.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/high-volume-tests/#using-cluster-keys-in-datastage-tests","title":"Using Cluster Keys in DataStage Tests","text":"<p>Defining a Cluster Key will cause DataStage to split the actual data output and expected data into multiple, smaller subsets before the data is compared.  Data is split such that each subset will only contain records that have the same values for all columns that make up the Cluster Key - a process somewhat analogous to DataStage partitioning.  The data are then sorted and a comparison of actual and expected data is performed using multiple, smaller operations which require less memory and are performed sequentially.  </p> <p>Test result behavior: Due to the iterative nature of comparisons using a Cluster Key, each record which has differences in the Cluster Key columns will be reported as 1 added record and 1 removed record rather than shown as a single record with a change indicator.</p> <p>A good Cluster Key is one that results in data subsets which strike a balance between the following factors:</p> <ul> <li>Each subset should fit in memory during comparison. Test execution will abort when memory thresholds are breached.</li> <li>Are as large as possible given the memory constraint. Lots of tiny subsets will degrade comparison performance.</li> </ul> <p>Selecting an appropriate Cluster Key might require several iterations to find a column (or combination of columns) which not only prevents Job aborts but also keeps run times acceptable.  Unless you are comparing unusually wide records, a good starting point is to aim for each subset of data to contain no more than 1,000 records and adjust the Cluster Key if memory thresholds continue to be breached.</p> <p>If you have used Interception to capture some input and / or expected test data for a DataStage Test and subsequently decide you want to apply a Cluster Key, you don\u2019t have to re-run Interception. This is also the case if you\u2019ve manually created any test data files. The Cluster Key is used at run time and therefore doesn\u2019t require any additional data preparation by the user.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/high-volume-tests/#example","title":"Example","text":"<p>Consider the situation where a DataStage Test has to compare several million financial transaction records with the following schema on the 'order_out' link of stage 'ODBC_order':</p> Column name SQL type Length Scale Nullable Transaction_Date Timestamp No Account_Id VarChar 20 No Type_Code VarChar 5 No Description VarChar Yes Amount Decimal 18 2 No <p>The test specification can be updated with a Cluster Key to enable iterative comparison of actual and expected test data.  In this example, <code>Account_Id</code> and <code>Type_Code</code> are defined as the compound Cluster Key:</p> <pre><code>{\n    \"then\": [\n        {\n            \"path\": \"ODBC_orders.csv\",\n            \"stage\": \"ODBC_order\",\n            \"link\": \"order_out\",\n            \"cluster\": [\n                \"Account_Id\",\n                \"Type_Code\"\n            ]\n        }\n    ],\n}\n</code></pre> <p>Note: Cluster Keys are specified on a per-link basis. DataStage flows with multiple output links can use any combination of clustered and non-clustered comparisons within a single test specification.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/high-volume-tests/#caveats","title":"Caveats","text":"<p>Cluster keys should be chosen to break Actual and Expected data into clusters which are small enough to fit in memory.</p> <p>Note that if a Unit Test detects a value difference in a column which is a cluster key column, then the Unit Test difference report (which would normally describe the difference as a \u2018modified\u2019 row when not using a cluster key) will now describe the difference as distinct \u2018added\u2019 and \u2018removed\u2019 entries.  </p> <p>As useful as Cluster Keys are, it\u2019s poor practice to simply apply them to every DataStage test that has to process high data volumes. You will almost certainly find combinations of flows and data volumes in your project where no Cluster Key will reduce the memory demands of a DataStage test enough to avoid Job aborts (See Unit Test throws OutOfMemoryError exception). In these situations you can manage your test data volumes by \u2026</p> <ul> <li>carefully selecting a subset of records from your data sources,</li> <li>using the DataStage's data fabrication features, or</li> <li>both of these approaches in combination.</li> </ul>","tags":["DATASTAGE","TESTING"]},{"location":"testing/migrating-datastage-tests-from-older-versions/","title":"Migrating test cases from older versions of DataStage","text":"<p>Users of IBM\u00ae DataStage\u00ae v11.x who built DataStage unit tests using MettleCI will be familiar with the YAML syntax used to specify those unit tests. Test cases for Cloud Pak DataStage are different in that they are specified using JSON rather than YAML.  Despite this syntactic difference, the options and structure of a test specification remain identical.</p> <p>Existing MettleCI unit tests are easily migrated into DataStage on Cloud Pak for Data using the <code>mettleci unittest migrate</code> command available in the MettleCI command line.  The documentation for this command describes the process you need to undertake to safely migrate your tests to Cloud Pak.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/row-count-comparisons/","title":"Row count comparisons","text":"<p>You can configure a DataStage\u00ae test case to only compare outputs' row counts, rather than the content of those rows, by setting the <code>checkRowCountOnly</code> property to true.</p> <pre><code>{\n    \"then\": [\n        {\n            \"path\": \"ODBC_orders.csv\",\n            \"stage\": \"ODBC_order\",\n            \"link\": \"order_out\",\n            \"checkRowCountOnly\": true\n        }\n    ],\n}\n</code></pre> <p>Note that the <code>checkRowCountOnly</code> property takes a boolean value which does not use quotes.</p> <p>The test case report containing a single cell comparing the expected and actual output row count (of the form <code>Expected-&gt;Actual</code>.)</p> <p></p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/selective-stubbing/","title":"Selective stubbing","text":"<p>The process of 'stubbing' involves using a fabricated version of an external data source (a 'stub') that returns specific, deterministic values or behaviors. Stubs can be used to test code that relies on external data sources that are not available at a given time or in a given environment, or which are non-deterministic.</p> <p>When you specify a DataStage\u00ae unit test you are defining stub data and telling DataStage which link(s) to stub with that data. There may be some instances where you wish to define a test case that only injects test data into some of your flow's input links, and allow other source stages to operate normally during a test execution. These input links which are not stubbed are not referenced in your test specification and will connect to their configured data source and retrieve data at runtime.</p> <p>To define a link which should not be stubbed simply omit the link from the <code>given</code> section of your test specification.</p> <p>For example \u2026</p> <pre><code>{\n    \"given\": [\n        {\n            \"path\": \"filePurchasesIn.csv\",\n            \"stage\": \"dsEX_Purchase\",\n            \"link\": \"inPurchase\" \n        }\n    ],\n    \"then\": [\n        {\n            \"path\": \"filePurchasesOut.csv\",\n            \"stage\": \"dsTR_Purchase\",\n            \"link\": \"outPurchase\"\n        }\n    ],\n    \"when\": {\n        \"data_intg_flow_ref\": \"blah-blah-blah\",  \n        \"parameters\": {\n        }\n    }\n}\n</code></pre> <p> </p> <p>Note: When defining test cases that use selective stubbing you should to exercise caution when deploying those test cases to downstream test environments.  Those environments will need to be configured to permit stages which have not been stubbed to retrieve data from their configured data sources.</p>","tags":["DATASTAGE","TESTING"]},{"location":"testing/test-datastage-sparse-lookup-stages/","title":"Test Cases with sparse lookups","text":"<p>Note: This page is specific to sparse lookups.  Lookup stages configured to use normal lookups do not need any special considerations for DataStage unit testing.</p>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-datastage-sparse-lookup-stages/#using-a-sparse-lookup-explicit-test-method","title":"Using a sparse Lookup ('Explicit' test method)","text":"<p>When designing DataStage flows using a lookup stage you configure the lookup to operate in normale or sparse mode by changing the lookup Lookup type in the Output tab of the database stage providing  the lookup reference.  When a DataStage flow featuring a sparse lookup is compiled executed, however, the  lookup stage is not used to perform the sparse lookup.  Instead, DataStage replaces the lookup stage with  the database operator which is responsible for reading input rows, looking up values from the database,  and producing output records.  It is for this reason that all database log messages in the DataStage Director  are attributed to the Lookup stage and why the Database stage never appears in the Monitor of the DataStage Director.</p> <p>Open image-20200129-024550.png</p> <p>To Unit Test job designs using Sparse Lookup stages the sparse lookup functionality needs to be explicitly  replaced with user-supplied Unit Test data:</p> <p>Open image-20221116-231138.png</p> <p>The most explicit way to configure Unit Testing to replace a Sparse Lookup with Test data is by adding the input  link to the then (expected outputs) clause of the Unit Test Spec and the output link to the given (supplied inputs)  clause of the Job\u2019s Unit Test Specification.  </p> <p>The CSV file specified in the then clause contains the data that will be be compared to the data flow of records arriving  at the input of the Sparse Lookup stage.  The data should describe what records are expected to be used to provide the  sparse  lookup\u2019s key columns.</p> <p>The CSV file specified in the given clause contains the data that will be become the data flow of records from the output  of the Sparse Lookup stage.  The data should simulate what would be produced by the real Sparse Lookup Stage if it had  actually processed the Unit Test input records against the real database reference source, however they don't have to.</p> <p>Open image-20200129-014439.png</p> <pre><code>given:\n  - stage: SparseLookup\n    link: Output\n    path: SparseLookup-Output.csv\nwhen:\n...\nthen:\n  - stage: SparseLookup\n    link: Input\n    path: SparseLookup-Input.csv\n</code></pre> <p>Open image-20200129-014525.png</p>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-datastage-sparse-lookup-stages/#sparse-lookup-stage-replace-method","title":"Sparse Lookup Stage ('Replace' method)","text":"<p>MettleCI Unit Test Harness version 1.1-379 and later provides a convenient alternative capability which uses the new Unit Test Specification spareseLookup and associated key clauses to replace the entire Sparse Lookup stage with Test Data while only requiring the user to supply Test Data for the reference link:</p> <p>Open image-20200129-014439.png</p> <p><pre><code>given:\n  - sparseLookup: SparseLookup\n    path: Database-Reference.csv\n    key:\n      - KEY_COLUMN_1\n      - KEY_COLUMN_2\n...\n\nOpen image-20221116-232511.png\n\n \n\nIn this mode the Sparse Lookup stage is replaced entirely with a Unit Test version of the sparse lookup which uses the specified Test Data as the lookup data.  It is provided as a convenient alternative to explicitly replacing Sparse Lookup stage input and output links, allowing the remaining logic in the job to be tested while not actually testing the behaviour of DataStage\u2019s Sparse Lookup stage.  \n\nWhen generating Unit Tests from Job designs containing Sparse Lookups, MettleCI Workbench version 1.0-1483 and later will automatically apply this Unit Test pattern and generate test specifications which replace Sparse Lookup stages entirely.\n\n## Known Limitations\n\nReplacing the Sparse Lookup stage with a Unit Test version of the sparse lookup comes with some limitations that DataStage developers should keep in mind.  \n\nUnit Test Sparse Lookup stages simulate typical key matching and assumes data equality with three-valued-logic semantics.  Custom key-matching logic embedded in custom lookup SQL (eg. SQL with a where clause like where KEY_COLUMN=Upper(ORCHESTRATE.KEY_COLUMN)) will not be replicated.\n\nWhen running Unit Testing in Interception mode, one additional Sort operation per Unit Test Sparse Lookup stage is required.  For sparse lookup stages which produce large volumes of output data this can have an adverse affect on job execution times when running in Interception mode.\n\nReference data records that contain Nulls or Default Values for all columns are ignored during interception when the Sparse Lookup stage is set to Continue on Lookup Failure.  This will have no functional impact on the output of the Sparse Lookup Stage and is expected behaviour.\n\nWhere you consider these limitations unacceptable you can revert to the original (explicit) method for replacing Sparse Lookups.\n\nA pragmatic approach for Multiple Sparse Lookups\nFor jobs where the vast majority of job logic is implemented using Sparse Lookup stages, replacing all lookups with Unit Test data would result in little-to-no DataStage logic being tested (as illustrated below).  \n\nOpen image-20200129-035608.png\n\nFor the type of Job design show above, a traditional explicit testing approach would necessitate the developer providing 8 sets of test data! An alternative testing approach is to leave the Sparse Lookups in place and replace only the input and output stages with Unit Test data.  A live Database connection will be required during testing but the when clause can be used to set job parameters that dictate database connection settings. \n\nTechnically this is an Integration Test, not a Unit Test: The Unit Test Harness does not provide any functionality for populating database reference tables with Unit Test data prior to test execution, users are responsible for managing Integration Test setup and tear down through governance and/or CI/CD pipeline customisation.\n\nOpen image-20200129-040758.png\n\n \n</code></pre> given:   - stage: Source     link: Input     path: Source-Output.csv when:   parameters:     DbName: MyUnitTestDb     DbUser: MyUnitTestUser     DbPass: {iisenc}dKANDae233DJIQeiidm== then:   - stage: Target     link: Output     path: Target-Output.csv ```</p> <p>Open image-20200129-040641.png</p>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-specification-format/","title":"DataStage test specification format","text":"<ul> <li>Structure overview</li> <li>Given these inputs</li> <li>Sparse Lookup sources</li> <li>When these conditions are met</li> <li>Then expect these outputs</li> </ul>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-specification-format/#structure","title":"Structure","text":"<p>A DataStage\u00ae test case specification (often abbreviated \u2018Spec') is a JSON-formatted file which uses a grammar modelled loosely on the Gherkin syntax used by the Cucumber testing tool. The overall structure follows the common Gherkin pattern \u2026</p> <pre><code>{\n    \"given\": [\n        { This test data on input link 1 },\n        { This test data on input link 2 }\n    ],\n    \"when\": {\n        I execute the test case with these options and parameter values\n    },\n    \"then\": [\n        { Expect this data to appear on output link 1 },\n        { Expect this data to appear on output link 2 }\n    ]\n}\n</code></pre> <p>Note: The user interface may order the JSON objects alphabetically (<code>given</code> &gt; <code>then</code> &gt; <code>when</code>) but this has no effect on the functionality of the test.</p>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-specification-format/#given","title":"Given","text":"<p>The <code>given</code> property array associates test data files with your flow's input , thereby defining the test values you wish to inject into your flow's inputs at runtime.</p> <p>For example:</p> <pre><code>{\n    \"given\": [\n        {\n            \"path\": \"fileCustomers.csv\",\n            \"stage\": \"sfCustomers\",\n            \"link\": \"Customers\" \n        },\n        {\n            \"path\": \"fileOrders.csv\",\n            \"stage\": \"sfOrders\",\n            \"link\": \"Orders\"\n        }\n    ],\n}\n</code></pre> <p>Some source stages can be configured with multiple output links so each input in your test specification's <code>given</code> property array is uniquely identified using a combination of the stage and link names to eliminate ambiguity.  The array also contains a <code>path</code> property to identify the test data CSV file containing the test data that is to be injected on each incoming link.</p> <p>Note that not every stage in a job must be provided with test data.  You can easily craft a test specification which uses test data for only a subset of flow stages.</p>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-specification-format/#sparse-lookup-sources","title":"Sparse Lookup sources","text":"<p>When an input source is used with a Sparse Lookup stage then rather than using the stage property to specify the input you will use the <code>sparseLookup</code> property.</p> <p>For example:</p> <pre><code>{\n    \"given\": [\n        {\n            \"path\": \"fileCustomers.csv\",\n            \"stage\": \"sfCustomers\",\n            \"link\": \"Customers\" \n        },\n        {\n            \"sparseLookup\": \"SparseLookup\",\n            \"path\": \"Database-Reference.csv\",\n            \"key\": [\n                \"KEY_COLUMN_1\",\n                \"KEY_COLUMN_2\"\n            ]\n        }\n    ],\n}\n</code></pre> <p>The <code>sparseLookup</code> property identifies a JSON object which specifies \u2026</p> <ul> <li>the value defining the name of the sparse lookup reference stage,</li> <li>a path to the relevant CSV test data file, and</li> <li>a list of key columns to be used for the sparse lookup.</li> </ul>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-specification-format/#when","title":"When","text":"<p>The <code>when</code> property array specifies which job will be executed during testing as well as any parameters (including job macros) that affect the data produced by the job.</p> <p>For example, this specification will</p> <p>Substitute hardcoded values for the <code>DSJobStartDate</code> and <code>DSJobStartTime</code> macros and the <code>paramStartKey</code> parameter:</p> <pre><code>{\n    \"when\": {\n        \"data_intg_flow_ref\": \"3023970f-ba2dfb02bd3a\",  \n        \"parameters\": {\n            \"DSJobStartDate\": \"2012-01-15\",\n            \"DSJobStartTime\": \"11:05:01\",\n            \"paramStartKey\": \"100\"\n        }\n    },\n}\n</code></pre> <p>One application of the <code>parameters</code> property is to supply values to make flows that rely on system date and time information produce a deterministic output by hard coding those values when testing.</p> <p>Note that the <code>data_intg_flow_ref</code> property is an internally-generated DataStage reference to the flow with which this test is associated and should not be changed.</p>","tags":["DataStage","Creating Tests"]},{"location":"testing/test-specification-format/#then","title":"Then","text":"<p>The <code>then</code> property array associates test data files with your flow's output links.</p> <pre><code>{\n    \"then\": [\n        {\n            \"path\": \"ODBC_customers.csv\",\n            \"stage\": \"ODBC_customer\",\n            \"link\": \"customer_out\"\n        },\n        {\n            \"path\": \"ODBC_orders.csv\",\n            \"stage\": \"ODBC_order\",\n            \"link\": \"order_out\"\n        }\n    ],\n}\n</code></pre> <p>Similar to the <code>given</code> property, because some target stages can be configured with multiple input links the test specification's <code>then</code> property array uniquely identifies links using a combination of the stage and link names. The array also contains a <code>path</code> property to identify the test data CSV file containing the expected test output that will be compared to the actual data appearing on each outgoing link.</p> <p>Other properties which extend the capabilities of your test case can be included in the <code>then</code> property array:</p> <ul> <li>The <code>ClusterKey</code> property: Improve performance of test cases when using data volumes</li> <li>The <code>checkRowCountOnly</code> property: Configure your tests to only count the number of rows</li> <li>The <code>ignore</code> property: Exclude specific columns from test comparisons</li> </ul>","tags":["DataStage","Creating Tests"]},{"location":"testing/testing-datastage-flows/","title":"Getting Started With Testing DataStage Flows","text":"<p>DataStage\u00ae test cases are design-time assets that use data files to define the inputs and expected outputs of your DataStage flows.</p> <p>The basic building blocks of a test case are:</p> <ul> <li>A test specification</li> <li>One or more test data input files</li> <li>One or more test data output files</li> </ul> <p>Each DataStage test case is associated with a single DataStage flow. You can create DataStage test cases as a new asset or directly from within the DataStage designer canvas.  A DataStage test case is executed by a job in a manner similar to its associated DataStage flow. During execution the input data files are injected into your flow's incoming links and the data appearing on the output links are compared to the output files containing the expected outputs. Any differences in the two will cause the test to fail and the differences to be reported alongside the test case's job log.  </p>","tags":["DataStage","Creating Tests","Executing Tests"]},{"location":"testing/testing-datastage-flows/#using-datastage-test-cases","title":"Using DataStage test cases","text":"<ol> <li>Configuring test data storage</li> <li>Creating DataStage test cases</li> <li>Editing DataStage tests</li> <li>Executing DataStage tests</li> <li>Verifying DataStage test results</li> <li>Migrating test cases from older versions of DataStage</li> </ol>","tags":["DataStage","Creating Tests","Executing Tests"]},{"location":"testing/troubleshoot-testing/","title":"Troubleshooting","text":"Problem Resolution I've modified my flow design and my test no longer passes Create a new test result baseline. The test case editor display the following message:<code>No connection has been defined on the project. Some features may be limited.</code> Configure a connection to store your test data.","tags":["DATASTAGE","TESTING"]},{"location":"testing/using-data-fabrication/","title":"Using Test Case Data Fabrication","text":"<p>Test Casse Data fabrication is a DataStage capability accessed via the unit test data editor.  This page provides a set of steps to using it.</p>","tags":["DataStage","Creating Tests","Data Fabrication"]},{"location":"testing/using-data-fabrication/#step-by-step-instructions","title":"Step-by-step instructions","text":"<ol> <li>When editing test data use the column overflow menu ( \u2807) to select edit column then under Data Generation Settings select your Generator Type.  The fabrication  tools listed, and their uses, are described in MettleCI\u2019s data fabrication tools.</li> </ol> Note <p>Note that some generators may prompt you for parameters which are available (or required) for you to customise their behaviour.  Where one or more parameters are mandatory the column editor panel on the right of the screen will not permit you to save your settings (and hence close the panel) until the mandatory parameters have been supplied.</p> <ol> <li>When you have selected your generator type and provided any required parameters, click Save.</li> <li>Back in the test data editor you can regenerate data using your supplied specification in one of three ways:<ol> <li>Right click the column header and select Regenerate data,</li> <li>Right click the table header (the unlabelled top-left header cell) and select Regenerate data, or</li> <li>Select a subset of cells by clicking and dragging in the table, then right click the table selection and select Regenerate data.</li> </ol> </li> </ol> <p>Note that these data fabrication settings are evaluated once, at the time that you select the Regenerate data option in the MettleCI Workbench test data editor. Once invoked they will populate the selected test data table cells with appropriate values.  The test data fabricaction settings will NOT be stored in the test data table\u2019s metadata definition and the data itself will remain static, even when depoyed to downstream systems.  The only way to regenerate the test data is to use the test data editory in a MettleCI Workbench instance.</p>","tags":["DataStage","Creating Tests","Data Fabrication"]},{"location":"testing/verifying-test-results/","title":"Verifying test results","text":"<p>You can verify DataStage\u00ae test results either from the CPD Cluster home page, the DataStage designer canvas, or the DataStage test case editor.</p>","tags":["DataStage","Test Results"]},{"location":"testing/verifying-test-results/#from-the-cpd-cluster-home-page","title":"From the CPD Cluster home page","text":"<ol> <li>Open an existing project and select the Jobs tab.</li> <li>Click the name of the test case you wish to inspect.  This will display the job details panel.</li> <li>Click the timestamp of a test to see its results.</li> </ol>","tags":["DataStage","Test Results"]},{"location":"testing/verifying-test-results/#from-the-datastage-canvas","title":"From the DataStage canvas","text":"<ol> <li>Open an existing DataStage flow for which you have created a test case.</li> <li>Click the Test cases icon to open up the test cases side panel.</li> <li>Click the timestamp of a test to see its results.</li> </ol>","tags":["DataStage","Test Results"]},{"location":"testing/verifying-test-results/#from-the-datastage-test-case-editor","title":"From the DataStage test case editor","text":"<ol> <li>Open an existing test case and select the Test history tab.</li> <li>Click the timestamp of a test to see its results.</li> </ol> <p>On the resulting Run details page select the Test results tab to see details about your test outcome.  Successful tests will present a simple acknowledgement message:</p> <p></p> <p>Test case errors will produce a difference report detailing how the expected and actual results differ from one another.</p> <p></p> <p>A difference report will be available for each failed test - i.e. each test where the expected and actual results varied.  For DataStage flows with multiple test failures (for multiple stubbed outputs) you can select which difference report to display by selecting the relevant output from the Stubbed link drop down box.</p>","tags":["DataStage","Test Results"]},{"location":"testing/verifying-test-results/#the-difference-report","title":"The difference report","text":"<p>Every DataStage test case involved the comparison of at least one Actual result set, produced by your Flow, and an associated Expected result set, defined by your test case. The differences in these result sets is expressed in a tabular form which describe, using indicators in row headers, column headers, or cells, the operations that would be required to modify the Expected data to match the Actual data.</p> <p>Taking the example test report above:</p> Change type Indicator Example Inserted rows An additional, unexpected row (for customer 'Ardith Beahan') is present Deleted rows The expected row (for customer 'Doc Brown') is missing Inserted columns An additional, unexpected INTEGER column CENTS was produced Deleted columns The expected TINYINT column MEMBERSHIP was not found Modified column metadata Additional header row A VARCHAR column was renamed from <code>FIRST_NAME</code> to <code>FirstName</code> as indicated by an additional header row Modified cell values A modified value (of the form <code>Expected-&gt;Actual</code>) in the DOLLARS columns for person 'Josianne Mante'","tags":["DataStage","Test Results"]}]}